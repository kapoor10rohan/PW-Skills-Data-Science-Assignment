{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7a5d11f-556a-4a3f-9968-c7d09ace7bb7",
   "metadata": {},
   "source": [
    "Q1. What is the purpose of grid search cv in machine learning, and how does it work?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3b3d7e-289e-4027-9485-65b332f52ff3",
   "metadata": {},
   "source": [
    "GridSearchCV is a technique for finding the optimal parameter values from a given set of parameters in a grid. It’s essentially a cross-validation technique. The model as well as the parameters must be entered. After extracting the best parameter values, predictions are made.\n",
    "\n",
    "We pass predefined values for hyperparameters to the GridSearchCV function. We do this by defining a dictionary in which we mention a particular hyperparameter along with the values it can take. GridSearchCV tries all the combinations of the values passed in the dictionary and evaluates the model for each combination using the Cross-Validation method. Hence after using this function we get accuracy/loss for every combination of hyperparameters and we can choose the one with the best performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7612db3c-c19f-4163-ab42-3c992dfafcd8",
   "metadata": {},
   "source": [
    "Q2. Describe the difference between grid search cv and randomize search cv, and when might you choose\n",
    "one over the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4df636e-941f-47fa-90bb-c714236a7a7d",
   "metadata": {},
   "source": [
    "Grid Search is one of the most basic hyper parameter technique used and so their implementation is quite simple. All possible permutations of the hyper parameters for a particular model are used to build models. The performance of each model is evaluated and the best performing one is selected. Since GridSearchCV uses each and every combination to build and evaluate the model performance, this method is highly computational expensive.\n",
    "\n",
    "In randomizedsearchcv, instead of providing a discrete set of values to explore on each hyperparameter, we provide a statistical distribution or list of hyper parameters. Values for the different hyper parameters are picked up at random from this distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14c0856-f1da-4d29-9067-73fe5fa7257f",
   "metadata": {},
   "source": [
    "Q3. What is data leakage, and why is it a problem in machine learning? Provide an example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a3be49-f390-4b5f-8bb2-fe4a6b7db535",
   "metadata": {},
   "source": [
    "A scenario when ML model already has information of test data in training data, but this information would not be available at the time of prediction, called data leakage. It causes high performance while training set, but perform poorly in deployment or production.\"\n",
    "\n",
    "Data leakage generally occurs when the training data is overlapped with testing data during the development process of ML models by sharing information between both data sets. Ideally, there should not be any interaction between these data sets (training and test sets). Still, sharing data between tests and training data sets is an accidental scenario that leads to the bad performance of the models. Hence, creating an ML predictive model always ensures that there is no overlapping between the training data and the testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d94b79-f80e-4ec2-b7bc-a526986d4c95",
   "metadata": {},
   "source": [
    "Q4. How can you prevent data leakage when building a machine learning model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311f2904-b5e4-4a97-ad4d-f0542e2d0226",
   "metadata": {},
   "source": [
    "Data leakage problems can be severe for any model prediction, but we can fix or avoid data leakage using tips and tricks.\n",
    "\n",
    "Extract the appropriate set of features\n",
    "\n",
    "Add an individual validation set.\n",
    "\n",
    "Apply data pre-processing separately to both data sets\n",
    "\n",
    "Time-series data\n",
    "\n",
    "Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a62bc5e-0a59-4ed6-9c76-d19d9917d2dd",
   "metadata": {},
   "source": [
    "Q5. What is a confusion matrix, and what does it tell you about the performance of a classification model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3b5ea7-909d-4726-85c7-1762c844d296",
   "metadata": {},
   "source": [
    "A confusion matrix is a table that is used to define the performance of a classification algorithm. A confusion matrix visualizes and summarizes the performance of a classification algorithm.\n",
    "\n",
    " It is often used to measure the performance of classification models, which aim to predict a categorical label for each input instance. The matrix displays the number of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN) produced by the model on the test data.\n",
    " \n",
    " Need for Confusion Matrix in Machine learning\n",
    "\n",
    "It evaluates the performance of the classification models, when they make predictions on test data, and tells how good our classification model is.\n",
    "\n",
    "It not only tells the error made by the classifiers but also the type of errors such as it is either type-I or type-II error.\n",
    "\n",
    "With the help of the confusion matrix, we can calculate the different parameters for the model, such as accuracy, precision, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e09619-64bd-4d43-a2bc-7fd1c87b9a5d",
   "metadata": {},
   "source": [
    "Q6. Explain the difference between precision and recall in the context of a confusion matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16eef931-c011-4b4e-9b9b-f3fdbaa5af6e",
   "metadata": {},
   "source": [
    "Precision: It can be defined as the number of correct outputs provided by the model or out of all positive classes that have predicted correctly by the model, how many of them were actually true. \n",
    "\n",
    "Recall: It is defined as the out of total positive classes, how our model predicted correctly. The recall must be as high as possible.\n",
    "\n",
    "Precision and recall are two evaluation metrics used to measure the performance of a classifier in binary and multiclass classification problems.\n",
    "Precision measures the accuracy of positive predictions, while recall measures the completeness of positive predictions.\n",
    "High precision and high recall are desirable, but there may be a trade-off between the two metrics in some cases.\n",
    "Precision and recall should be used together with other evaluation metrics, such as accuracy and F1-score, to get a comprehensive understanding of the performance of a classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a301561-66db-421e-8adf-3509d8fdf5f6",
   "metadata": {},
   "source": [
    "Q7. How can you interpret a confusion matrix to determine which types of errors your model is making?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1072ed04-6240-4784-90d9-a0dd4d5ce7f1",
   "metadata": {},
   "source": [
    "\n",
    "True Positive (TP) \n",
    "\n",
    "The predicted value matches the actual value, or the predicted class matches the actual class.\n",
    "The actual value was positive, and the model predicted a positive value.\n",
    "True Negative (TN) \n",
    "\n",
    "The predicted value matches the actual value, or the predicted class matches the actual class.\n",
    "The actual value was negative, and the model predicted a negative value.\n",
    "False Positive (FP) – Type I Error\n",
    "\n",
    "The predicted value was falsely predicted.\n",
    "The actual value was negative, but the model predicted a positive value.\n",
    "Also known as the type I error.\n",
    "False Negative (FN) – Type II Error\n",
    "\n",
    "The predicted value was falsely predicted.\n",
    "The actual value was positive, but the model predicted a negative value.\n",
    "Also known as the type II error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e77256-1fa7-457d-b934-ee9fb9e6970d",
   "metadata": {},
   "source": [
    "Q8. What are some common metrics that can be derived from a confusion matrix, and how are they\n",
    "calculated?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1710690-ccef-4342-a16e-65239549ea5b",
   "metadata": {},
   "source": [
    "Accuracy:\n",
    "The formula for calculating accuracy, based on the chart above, is (TP+TN)/(TP+FP+FN+TN) or all true positive and true negative cases divided by the number of all cases.\n",
    "Accuracy is commonly used to judge model performance, however, there are a few drawbacks that must be considered before using accuracy liberally. One of these drawbacks deals with unbalanced datasets where one class, either true or false, is more common than the other causing the model to classify observations based on this imbalance. For example, if 90% of cases are false and only 10% are true, there’s a very high possibility of our model having an accuracy score of around 90%. Naively, it may seem like we a have high rate of accuracy, but in actuality, we are just 90% likely to predict the ‘false’ class, so we don’t actually have a good metric. Normally, I wouldn’t use accuracy as a performance metric, I’d rather use precision, recall, or the F1 score.\n",
    "\n",
    "Precision:\n",
    "Precision is the measure of true positives over the number of total positives predicted by your model. The formula for precision can be written as: TP/(TP+FP). What this metric allows you to calculate is the rate of which your positive predictions are actually positive.\n",
    "\n",
    "Recall:\n",
    "Recall (a.k.a sensitivity) is the measure of your true positive over the count of actual positive outcomes. The formula for recall can be expressed as: TP/(TP+FN). Using this formula, we can assess how well our model is able to identify the actual true result.\n",
    "\n",
    "F1 Score:\n",
    "The F1 score is the harmonic mean between precision and recall. The formula for the F1 score can be expressed as: 2(p*r)/(p+r) where ‘p’ is precision and ‘r’ is recall. This score can be used as an overall metric that incorporates both precision and recall. The reason we use the harmonic mean as opposed to the regular mean, is that the harmonic mean punishes values that are further apart.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0280538-ccac-4bc6-b7dc-2dd46a90b842",
   "metadata": {},
   "source": [
    "Q9. What is the relationship between the accuracy of a model and the values in its confusion matrix?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d17e3d-0ec5-444c-abfa-cc5bbfd4d87b",
   "metadata": {},
   "source": [
    "The formula for calculating accuracy, based on the chart above, is (TP+TN)/(TP+FP+FN+TN) or all true positive and true negative cases divided by the number of all cases.\n",
    "Accuracy is commonly used to judge model performance, however, there are a few drawbacks that must be considered before using accuracy liberally. One of these drawbacks deals with unbalanced datasets where one class, either true or false, is more common than the other causing the model to classify observations based on this imbalance. For example, if 90% of cases are false and only 10% are true, there’s a very high possibility of our model having an accuracy score of around 90%. Naively, it may seem like we a have high rate of accuracy, but in actuality, we are just 90% likely to predict the ‘false’ class, so we don’t actually have a good metric. Normally, I wouldn’t use accuracy as a performance metric, I’d rather use precision, recall, or the F1 score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734e21fa-44d5-4b0b-b808-19f9ae453b0d",
   "metadata": {},
   "source": [
    "Q10. How can you use a confusion matrix to identify potential biases or limitations in your machine learning\n",
    "model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c040998-29fb-4d7d-b717-7594ece03254",
   "metadata": {},
   "source": [
    "Confusion matrices reveal when a model consistently confuses two classes, making it simple to determine how reliable a model's results are likely to be. The effectiveness of a classification model, enabling business users to identify which data their model might be unable to accurately categorize. When applying insights or predictions from the model to real-world business choices, this knowledge is important.\n",
    "\n",
    "For instance, there is a significantly different outcome when a model predicts that a credit investment opportunity would result in default when it really didn't (false positive) than when the lender unintentionally advances a loan that actually results in a default (false negative). The user should use an alternative model or manually tune their current model if they can see from the confusion matrix that their model is likely to produce false negatives for the loan dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
