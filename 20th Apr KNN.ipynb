{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c86fdaab-879a-4c56-89ba-66b0fab50c96",
   "metadata": {},
   "source": [
    "Q1. What is the KNN algorithm?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9563cb9-13f1-4e42-a506-e26183a1f80d",
   "metadata": {},
   "source": [
    "K-NN algorithm assumes the similarity between the new case/data and available cases and put the new case into the category that is most similar to the available categories.\n",
    "K-NN algorithm stores all the available data and classifies a new data point based on the similarity. This means when new data appears then it can be easily classified into a well suite category by using K- NN algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22975068-cb8e-4a05-a609-22eecb2fefc0",
   "metadata": {},
   "source": [
    "Q2. How do you choose the value of K in KNN?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921aa897-c97c-4731-998f-4b91be0bc7ef",
   "metadata": {},
   "source": [
    "There is no particular way to determine the best value for \"K\", so we need to try some values to find the best out of them. The most preferred value for K is 5.\n",
    "A very low value for K such as K=1 or K=2, can be noisy and lead to the effects of outliers in the model.\n",
    "Large values for K are good, but it may find some difficulties."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f568db-1f59-4067-b40c-740e764f620a",
   "metadata": {},
   "source": [
    "Q3. What is the difference between KNN classifier and KNN regressor?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a42d46-0f56-4de2-a256-b2b7377e100b",
   "metadata": {},
   "source": [
    "KNN regression tries to predict the value of the output variable by using a local average.\n",
    "KNN classification attempts to predict the class to which the output variable belong by computing the local probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5646b753-9966-4706-8cbf-fe33501d6b6d",
   "metadata": {},
   "source": [
    "Q4. How do you measure the performance of KNN?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc798178-5907-4792-b3e9-bd3c3c932dbc",
   "metadata": {},
   "source": [
    "The k-nearest neighbour classification (k-NN) is one of the most popular distance-based algorithms. This classification is based on measuring the distances between the test sample and the training samples to determine the final classification output. The traditional k-NN classifier works naturally with numerical data. \n",
    "\n",
    "Confusion matrix\n",
    "The performance measures used to analyse the results are academically renowned and revolve around the usage of the confusion matrix28. Figure 2 presents the visual for the matrix. The matrix is the amalgamation of results from classifications and has four primary attributes that present the result data. If the classification is predicted to be 1 and the true value is 1, the result is classified as true positive (TP). The same principle revolves around the value 0 and is classified as true negative (TN). When the prediction is 1 and the true value is 0, the result is classified as false positive (FP), with the inverse being called false negative (FN).\n",
    "\n",
    "\n",
    "In this research, the confusion matrix is used to create three performance measures: accuracy, precision and recall.\n",
    "\n",
    "The accuracy measure is calculated by taking all the true predictions and dividing them among all the predicted values, including the true predictions.\n",
    "\n",
    "ğ´ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦=ğ‘‡ğ‘ƒ+ğ‘‡ğ‘ğ‘‡ğ‘ƒ+ğ‘‡ğ‘+ğ¹ğ‘ƒ+ğ¹ğ‘\n",
    "where TP, TN, FP and FN are the true positive, true negative, false positive and false negative cases of the result data, respectively.\n",
    "\n",
    "The precision measure is calculated by taking the true positive values and dividing them among both true and false positive values.\n",
    "\n",
    "ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›=ğ‘‡ğ‘ƒğ‘‡ğ‘ƒ+ğ¹ğ‘ƒ\n",
    "where TP and FP are the true positive and false positive cases of the result data, respectively.\n",
    "\n",
    "The recall measure is calculated similarly to the precision measure by taking the true positive values and dividing them among the true positive and false negative values.\n",
    "\n",
    "ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™=ğ‘‡ğ‘ƒğ‘‡ğ‘ƒ+ğ¹ğ‘\n",
    "where TP and FN are the true positive and false negative cases of the result data, respectively.\n",
    "\n",
    "These three performance measures will be used to assess the classification results of the variants implemented in this paper. The complete set of these three performance measures will be used to create a new measure that will be discussed in the next section.\n",
    "\n",
    "Relative performance index (RPI)\n",
    "The relative performance index is a breakthrough assessor that collects data results of any other measure (accuracy, precision and recall, etc.) and produces a probabilistic result for the final assessment. The new performance measure that is being proposed here is inspired by another RPI measure proposed by Nagle29. The author proposed a new probabilistic calculation that removes bias by considering the range of results produced by a particular field and extracting the number of times the results of that field were above other fields. The RPI for a field is calculated using the extracted values and the number of fields that exist.\n",
    "\n",
    "The following equation describes the new performance measure of this study:\n",
    "\n",
    "ğ‘…ğ‘’ğ‘™ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’ğ‘ƒğ‘’ğ‘Ÿğ‘“ğ‘œğ‘Ÿğ‘šğ‘ğ‘›ğ‘ğ‘’ğ¼ğ‘›ğ‘‘ğ‘’ğ‘¥(ğ‘…ğ‘ƒğ¼)=âˆ‘ğ‘–=1ğ‘‘(ğ‘ğ‘–âˆ’ğ‘âˆ—ğ‘–)ğ‘‘\n",
    "where ğ‘âˆ—ğ‘– is the minimum accuracy/precision/recall value among all variants for dataset ğ‘–, ğ‘ğ‘– is the accuracy/precision/recall value for the variant under consideration for dataset ğ‘–, and ğ‘‘ is the number of datasets considered in this study.\n",
    "\n",
    "A higher RPI value indicates prediction superiority considering the underlying performance measures (e.g., accuracy and precision) and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1149408a-d27c-458e-9289-c4ea069507c4",
   "metadata": {},
   "source": [
    "Q5. What is the curse of dimensionality in KNN?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bcef6d-28ea-46a6-995a-a1df33ae1528",
   "metadata": {},
   "source": [
    "Distances between points\n",
    "\n",
    "The kNN classifier makes the assumption that similar points share similar labels. Unfortunately, in high dimensional spaces, points that are drawn from a probability distribution, tend to never be close together. \n",
    "\n",
    "Distances to hyperplanes\n",
    "\n",
    "So the distance between two randomly drawn data points increases drastically with their dimensionality. How about the distance to a hyperplane? Consider the following figure. There are two blue points and a red hyperplane. The left plot shows the scenario in 2d and the right plot in 3d. As long as d=2, the distance between the two points is âˆšÎ”x2+Î”y2. When a third dimension is added, this extends to âˆšÎ”x2+Î”y2+Î”z2, which must be at least as large (and is probably larger). This confirms again that pairwise distances grow in high dimensions. On the other hand, the distance to the red hyperplane remains unchanged as the third dimension is added. The reason is that the normal of the hyper-plane is orthogonal to the new dimension. This is a crucial observation. In d dimensions, dâˆ’1 dimensions will be orthogonal to the normal of any given hyper-plane. Movement in those dimensions cannot increase or decrease the distance to the hyperplane the points just shift around and remain at the same distance. As distances between pairwise points become very large in high dimensional spaces, distances to hyperplanes become comparatively tiny. \n",
    "\n",
    "Data with low dimensional structure\n",
    "Data may lie in low dimensional subspace or on sub-manifolds. Example: natural images (digits, faces). Here, the true dimensionality of the data can be much lower than its ambient space. The next figure shows an example of a data set sampled from a 2-dimensional manifold (i.e. a surface in space), that is embedded within 3d. Human faces are a typical example of an intrinsically low dimensional data set. Although an image of a face may require 18M pixels, a person may be able to describe this person with less than 50 attributes (e.g. male/female, blond/dark hair, ...) along which faces vary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e27554-13ac-4039-a84d-94df78267b10",
   "metadata": {},
   "source": [
    "Q6. How do you handle missing values in KNN?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b91cf5a-445f-4478-857e-50c20c5664ce",
   "metadata": {},
   "source": [
    "Although any one among a range of different models can be used to predict the missing values, the k-nearest neighbor (KNN) algorithm has proven to be generally effective, often referred to as â€œnearest neighbor imputation.â€\n",
    "\n",
    "An effective approach to data imputing is to use a model to predict the missing values. A model is created for each feature that has missing values, taking as input values of perhaps all other input features.\n",
    "\n",
    "If input variables are numeric, then regression models can be used for prediction, and this case is quite common. A range of different models can be used, although a simple k-nearest neighbor (KNN) model has proven to be effective in experiments. The use of a KNN model to predict or fill missing values is referred to as â€œNearest Neighbor Imputationâ€ or â€œKNN imputation.â€\n",
    "\n",
    "Configuration of KNN imputation often involves selecting the distance measure (e.g. Euclidean) and the number of contributing neighbors for each prediction, the k hyperparameter of the KNN algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b297509-6646-4f52-ae23-9b59c744b34a",
   "metadata": {},
   "source": [
    "Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for\n",
    "which type of problem?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e1d54c-45e2-4650-af47-7d479a9a693f",
   "metadata": {},
   "source": [
    "KNN regression tries to predict the value of the output variable by using a local average.\n",
    "KNN classification attempts to predict the class to which the output variable belong by computing the local probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674f7403-a735-4df4-b80f-c9193960465e",
   "metadata": {},
   "source": [
    "Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks,\n",
    "and how can these be addressed?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170439ca-8949-4412-ac2e-bb0d07e8d7aa",
   "metadata": {},
   "source": [
    "Strengths of KNN\n",
    "\n",
    "KNN is a simple and intuitive algorithm that is easy to understand and implement.\n",
    "KNN is a non-parametric algorithm that does not assume any particular distribution of the data.\n",
    "KNN can perform well on small datasets, as it can capture complex relationships between features and target variables without making assumptions about the underlying distribution of the data.\n",
    "Weaknesses of KNN\n",
    "\n",
    "KNN can be computationally expensive, especially as the number of features or data points grows, since it requires calculating the distance between each data point in the dataset.\n",
    "KNN can be sensitive to the choice of the number of neighbors (k) and the distance metric used to calculate the distances between data points.\n",
    "KNN can be affected by the curse of dimensionality, where the performance of the algorithm degrades as the number of features increases, due to the increasing sparsity of the feature space.\n",
    "KNN cannot handle missing values in the data, and may require imputation or removal of such values.\n",
    "To address some of the weaknesses of KNN, several modifications and extensions of the algorithm have been proposed. These include:\n",
    "\n",
    "Using distance-weighted voting, where closer neighbors have a greater influence on the final prediction.\n",
    "Using feature selection or dimensionality reduction techniques to reduce the number of features and improve the performance of the algorithm.\n",
    "Using ensemble techniques such as bagging or boosting to combine multiple KNN models and improve the accuracy and stability of the predictions.\n",
    "Using imputation techniques to handle missing values in the data.\n",
    "Overall, whether KNN classifier or regressor is better depends on the specific problem at hand. KNN classifier is suitable for problems where the target variable is categorical, while KNN regressor is suitable for problems where the target variable is continuous. The choice between the two also depends on the size and complexity of the dataset, as well as the performance requirements and computational resources available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820cf42f-2b36-4288-b69d-37c06f3a421c",
   "metadata": {},
   "source": [
    "Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57021507-4141-4046-bfc6-531445cb7ad6",
   "metadata": {},
   "source": [
    "Euclidean distance and Manhattan distance are two commonly used distance metrics in KNN algorithm. The main difference between them is the way they measure the distance between two points.\n",
    "\n",
    "Euclidean distance is the straight-line distance between two points in a Euclidean space. It is calculated as the square root of the sum of the squared differences between the corresponding elements of the two vectors. \n",
    "\n",
    "Manhattan distance, also known as Taxicab distance or L1 distance, is the sum of the absolute differences between the corresponding elements of the two vectors. It is called Manhattan distance because it is analogous to the distance a car would travel on a rectangular grid of streets to reach from one point to another. \n",
    "\n",
    "The main difference between Euclidean distance and Manhattan distance is that Euclidean distance is sensitive to the magnitude of the differences between the corresponding elements, while Manhattan distance is only sensitive to the direction of the differences. This means that in high-dimensional spaces, Euclidean distance may become less meaningful because the differences between the corresponding elements become increasingly small. In such cases, Manhattan distance may be a better choice.\n",
    "\n",
    "In summary, Euclidean distance is generally used when the magnitude of the differences between corresponding elements is important, while Manhattan distance is used when only the direction of the differences is important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6429a791-ac71-41a0-834f-8236b351d6d6",
   "metadata": {},
   "source": [
    "Q10. What is the role of feature scaling in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758d389c-6d8b-45a9-8dd8-eae592f6e470",
   "metadata": {},
   "source": [
    " KNN chooses the k closest neighbors and then based on these neighbors, assigns a class (for classification problems) or predicts a value (for regression problems) for a new observation\n",
    "    All such distance based algorithms are affected by the scale of the variables. \n",
    "    \n",
    "Feature scaling is an important preprocessing step in KNN. In KNN, the distance between two data points is calculated using the Euclidean or Manhattan distance formula. The distance calculation is sensitive to the scale of the features. Features with larger magnitudes will have a greater influence on the distance calculation than features with smaller magnitudes. This can lead to incorrect classifications or regressions.\n",
    "\n",
    "Feature scaling involves scaling the features to a common scale so that the distance calculation is not biased towards features with larger magnitudes. Common methods for feature scaling include normalization and standardization.\n",
    "\n",
    "Normalization scales the features to a range between 0 and 1, while standardization scales the features to have a mean of 0 and a standard deviation of 1. Both methods can improve the performance of KNN by making the distance calculation more accurate and reducing the impact of features with larger magnitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c80cf2-800e-44c2-adb4-aad88189b033",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
