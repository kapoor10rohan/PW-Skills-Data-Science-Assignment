{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "870a3cae-b494-4036-aa3b-b4a28e93f8f9",
   "metadata": {},
   "source": [
    "Q1. How does bagging reduce overfitting in decision trees?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8b07b2-8402-4ca2-9be5-f30b9bae1af5",
   "metadata": {},
   "source": [
    "By combining multiple models, bagging helps reduce the model’s variance and can prevent overfitting by introducing diversity into the training process. It is commonly used with decision trees but can also be applied to other models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3830ca9c-dbe8-4f65-91d0-023d22441b00",
   "metadata": {},
   "source": [
    "Q2. What are the advantages and disadvantages of using different types of base learners in bagging?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8ce143-e212-4604-b8b7-481fb016c811",
   "metadata": {},
   "source": [
    "The advantage of using different types of base learners in bagging is that it can help to reduce the correlation between the base models and improve the diversity of the ensemble. This can lead to a reduction in the variance of the model and improve the overall performance. The disadvantage is that it can increase the complexity of the model and increase the risk of overfitting. It is important to choose base learners that are diverse and complementary to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4fc926-bbf9-40f7-a5ff-512a41914704",
   "metadata": {},
   "source": [
    "Q3. How does the choice of base learner affect the bias-variance tradeoff in bagging?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ce9a35-ac66-49b6-9e7b-c369ad9b2967",
   "metadata": {},
   "source": [
    "The choice of base learner can have a significant impact on the bias-variance tradeoff in bagging. In general, using a complex base learner with high variance (e.g., a decision tree with many levels) can lead to a reduction in bias but an increase in variance. On the other hand, using a simple base learner with low variance (e.g., a decision stump with only one split) can lead to a reduction in variance but an increase in bias.\n",
    "\n",
    "Bagging works by reducing the variance of the model by averaging over many models, so it tends to work well with base learners that have high variance. This is because the average of many high-variance models can help to cancel out individual model errors, leading to an overall reduction in variance without a significant increase in bias.\n",
    "\n",
    "However, if the base learner is too complex and overfits to the training data, bagging may not be able to fully reduce the variance of the model. In this case, using a simpler base learner may be more effective at reducing variance and improving overall performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394f40de-d59a-452f-b095-4990dfb6afac",
   "metadata": {},
   "source": [
    "Q4. Can bagging be used for both classification and regression tasks? How does it differ in each case?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41634cf-cb5f-4340-9db5-f1ce2976d50c",
   "metadata": {},
   "source": [
    "Yes, bagging can be used for both classification and regression tasks.\n",
    "\n",
    "In classification, bagging can be used with a base classifier that predicts class labels. The final prediction is obtained by majority voting of the predictions of the base classifiers. Bagging can help reduce overfitting in the base classifiers, resulting in a more generalized classifier.\n",
    "\n",
    "In regression, bagging can be used with a base regressor that predicts continuous values. The final prediction is obtained by averaging the predictions of the base regressors. Similar to classification, bagging can help reduce overfitting in the base regressors, resulting in a more generalized regressor.\n",
    "\n",
    "The main difference between the two cases is in the way the final prediction is obtained. In classification, it is based on majority voting, while in regression it is based on averaging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b5b4d6-6190-4460-b064-e32342de7d68",
   "metadata": {},
   "source": [
    "Q5. What is the role of ensemble size in bagging? How many models should be included in the ensemble?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3314fff-994a-4e17-b7af-b732a6954cbd",
   "metadata": {},
   "source": [
    "The ensemble size is an important hyperparameter in bagging as it determines the number of models that will be trained and aggregated to form the final ensemble model. In general, increasing the ensemble size can lead to better performance up to a certain point, after which the performance may plateau or even degrade.\n",
    "\n",
    "The ideal number of models to include in the ensemble depends on several factors, including the size of the dataset, the complexity of the problem, and the computational resources available. In practice, it is common to start with a small ensemble size and gradually increase it while monitoring performance on a validation set, until no further improvement is observed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f354a5-e442-492b-8c71-53cd70d38761",
   "metadata": {},
   "source": [
    "Q6. Can you provide an example of a real-world application of bagging in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409274de-4cba-4ec6-a0d3-dcc3a99b45a0",
   "metadata": {},
   "source": [
    "The relationship between temperature and ozone in this data set is apparently non-linear, based on the scatter plot. Instead of building a single prediction model from the complete data set, 100 samples of the data were drawn. Each sample is different from the original data set, yet resembles it in distribution and variability. Predictions from these 100 were then samples made across the range of the data. The first 10 predicted smooth fits appear as grey lines in the figure below. The lines are clearly very wiggly and they overfit the data — a result of the bandwidth being too small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a88713d-2795-423b-9f00-534b1b836fcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
