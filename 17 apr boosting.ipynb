{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bf77635-da6c-4fd0-9e64-01489909ebf1",
   "metadata": {},
   "source": [
    "Q1. What is Gradient Boosting Regression?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1202a7c-2325-4c02-8dce-97c4cfd1710d",
   "metadata": {},
   "source": [
    "Gradient Boosting is a powerful boosting algorithm that combines several weak learners into strong learners, in which each new model is trained to minimize the loss function such as mean squared error or cross-entropy of the previous model using gradient descent. In each iteration, the algorithm computes the gradient of the loss function with respect to the predictions of the current ensemble and then trains a new weak model to minimize this gradient. The predictions of the new model are then added to the ensemble, and the process is repeated until a stopping criterion is met.\n",
    "\n",
    "We initially predict the average (since it’s regression here) of the y-column and build a decision tree based on that value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0396cb9d-58bf-4bb7-a6e4-40c3b191291f",
   "metadata": {},
   "source": [
    "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
    "simple regression problem as an example and train the model on a small dataset. Evaluate the model's\n",
    "performance using metrics such as mean squared error and R-squared.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b3be707-b096-4419-a96e-f39f6e239459",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # initialize the residuals with the target values\n",
    "        residuals = y.copy()\n",
    "\n",
    "        # loop over the number of estimators\n",
    "        for i in range(self.n_estimators):\n",
    "            # fit a decision tree to the residuals\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "\n",
    "            # predict the residuals for the current tree\n",
    "            residuals_pred = tree.predict(X)\n",
    "\n",
    "            # update the residuals with the predictions\n",
    "            residuals -= self.learning_rate * residuals_pred\n",
    "\n",
    "            # add the current tree to the list of trees\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # initialize the predictions with zeros\n",
    "        y_pred = np.zeros(X.shape[0])\n",
    "\n",
    "        # loop over the trees and add the predictions\n",
    "        for tree in self.trees:\n",
    "            y_pred += self.learning_rate * tree.predict(X)\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04fafb0a-ca10-470c-a635-1a9d81de3862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 623.4539375433474\n",
      "R-squared: 0.823539706892307\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "# generate a random regression problem\n",
    "X, y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=1)\n",
    "\n",
    "# split the data into training and testing sets\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=4)\n",
    "\n",
    "# train the gradient boosting regressor\n",
    "gb = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the testing set\n",
    "y_pred = gb.predict(X_test)\n",
    "\n",
    "# calculate mean squared error and R-squared\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"MSE:\", mse)\n",
    "print(\"R-squared:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f7e333-220e-4f4e-9257-4faa60f3b880",
   "metadata": {},
   "source": [
    "Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to\n",
    "optimise the performance of the model. Use grid search or random search to find the best\n",
    "hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f26e565c-0f74-4f77-b05c-ba22f1c10b81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingRegressor()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "X, y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=1)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=5)\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "model = GradientBoostingRegressor()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c66e4337-57cd-4b9b-9d0e-fb7629cdd39c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.655427488553789"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7e9d07-3e80-4348-b1cd-cc2a03335eaf",
   "metadata": {},
   "source": [
    "Q4. What is a weak learner in Gradient Boosting?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9522ee35-3606-446f-8bbe-906546c554e8",
   "metadata": {},
   "source": [
    "The term Weak Learner refers to simple models that do only slightly better than random chance. Boosting algorithms start with a single weak learner (tree methods are overwhelmingly used here), but technically, any model will do."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56228f6-558a-48df-8358-8f5651a3500b",
   "metadata": {},
   "source": [
    "Q5. What is the intuition behind the Gradient Boosting algorithm?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28568afb-2e29-4c75-8e0d-22b076195425",
   "metadata": {},
   "source": [
    "The intuition behind the Gradient Boosting algorithm is to iteratively improve a model by focusing on the residuals (or errors) of the previous model. It works by adding new weak models (often decision trees) to the ensemble, with each model trained to correct the errors made by the previous models.\n",
    "\n",
    "In the beginning, the Gradient Boosting algorithm trains the first model on the entire dataset, then the subsequent models are trained on the residuals of the previous model. The algorithm continues to iteratively minimize the residuals by adding new models to the ensemble until a stopping criterion is met, such as reaching a maximum number of models or achieving a certain level of performance.\n",
    "\n",
    "The final model is a weighted sum of all the weak models, where the weights are determined by the performance of each weak model. The Gradient Boosting algorithm is known to be a powerful and flexible method for both regression and classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad028dd6-cdd5-4549-853d-22e47d835d27",
   "metadata": {},
   "source": [
    "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273e7ac2-793f-4bd8-99c0-ccd04118984f",
   "metadata": {},
   "source": [
    "\n",
    "Gradient Boosting algorithm builds an ensemble of weak learners in a stage-wise manner. It starts by fitting a single weak learner (usually a decision tree) to the training data and calculating the residuals of the predictions. The next weak learner is then fitted to these residuals, rather than the original target values, with the aim of reducing the error made by the previous learner. This process is repeated iteratively, with each subsequent learner fitted to the residuals of the previous ensemble of learners, until a predefined number of learners has been fitted or a convergence criterion has been met.\n",
    "\n",
    "Each weak learner is fit to the gradient of the loss function with respect to the predictions of the previous ensemble of learners. This means that each subsequent learner is fitted to the negative gradient of the loss function with respect to the predictions made by the current ensemble of learners, which ensures that the next learner tries to correct the mistakes made by the previous learner.\n",
    "\n",
    "The final ensemble of weak learners is then combined to make predictions on new data. The combination is typically a weighted sum of the predictions made by each weak learner, with the weights determined by the performance of each learner on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233f3a3e-46d8-46ef-be6b-4e2b63a32f0f",
   "metadata": {},
   "source": [
    "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting\n",
    "algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a6d1d6-bbe9-4015-86d7-a6b5d5470c26",
   "metadata": {},
   "source": [
    "1. We start by assuming a function to approximate the target variable. This function is usually set to a constant value which is the mean of the target variable.\n",
    "2. We then calculate the errors by taking the difference between the actual target variable and the predicted values from step 1.\n",
    "3. We fit a new model, called a weak learner, to the errors. This model is typically a decision tree with a small depth.\n",
    "4. We add the predictions from this weak learner to our previous approximation, with a shrinkage parameter that controls the contribution of the weak learner. The new function becomes a better approximation of the target variable.\n",
    "5. We repeat steps 2-4 until a stopping criterion is met, such as a maximum number of iterations or a minimum improvement in performance.\n",
    "6. Finally, we obtain the final model by combining the weak learners with their corresponding shrinkage parameters, and use it to make predictions on new data.\n",
    "By repeating this process, the algorithm learns to gradually improve the approximation of the target variable, using the residuals of the previous model as the new target variable for the next model. This approach allows the algorithm to build a highly accurate model, even with complex, non-linear relationships between the input and output variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51e7c50-e6b6-49e3-b57a-0917470000d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
