{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4791871-3dcb-40cc-8e36-64917482606e",
   "metadata": {},
   "source": [
    "Q1. What is the main difference between the Euclidean distance metric and the Manhattan distance\n",
    "metric in KNN? How might this difference affect the performance of a KNN classifier or regressor?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d2e7bd-2106-4ec7-91bc-7413654da8a7",
   "metadata": {},
   "source": [
    "Euclidean distance is the straight line distance between 2 data points in a plane.\n",
    "\n",
    "It is calculated using the Minkowski Distance formula by setting ‘p’ value to 2, thus, also known as the L2 norm distance metric.\n",
    "\n",
    "This formula is similar to the Pythagorean theorem formula, Thus it is also known as the Pythagorean Theorem.\n",
    "\n",
    "We use Manhattan distance, also known as city block distance, or taxicab geometry if we need to calculate the distance between two data points in a grid-like path.\n",
    "\n",
    "Manhattan Distance is preferred over the Euclidean distance metric as the dimension of the data increases. This occurs due to something known as the ‘curse of dimensionality’."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5091487-39c8-47cc-b6e3-cc3273e2211f",
   "metadata": {},
   "source": [
    "Q2. How do you choose the optimal value of k for a KNN classifier or regressor? What techniques can be\n",
    "used to determine the optimal k value?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c561d77-bcfb-4994-b922-ea8513420793",
   "metadata": {},
   "source": [
    "The k value in the k-NN algorithm defines how many neighbors will be checked to determine the classification of a specific query point. For example, if k=1, the instance will be assigned to the same class as its single nearest neighbor. Defining k can be a balancing act as different values can lead to overfitting or underfitting. Lower values of k can have high variance, but low bias, and larger values of k may lead to high bias and lower variance. The choice of k will largely depend on the input data as data with more outliers or noise will likely perform better with higher values of k. Overall, it is recommended to have an odd number for k to avoid ties in classification, and cross-validation tactics can help you choose the optimal k for your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c17be3d-f666-4deb-846f-bc8d51c7051b",
   "metadata": {},
   "source": [
    "Q3. How does the choice of distance metric affect the performance of a KNN classifier or regressor? In\n",
    "what situations might you choose one distance metric over the other?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ee114f-5540-4fa6-8fbc-45a0e3bc1603",
   "metadata": {},
   "source": [
    "KNN is the most commonly used and one of the simplest algorithms for finding patterns in classification and regression problems. It is an unsupervised algorithm and also known as lazy learning algorithm. It works by calculating the distance of 1 test observation from all the observation of the training dataset and then finding K nearest neighbors of it. This happens for each and every test observation and that is how it finds similarities in the data. For calculating distances KNN uses a distance metric from the list of available metrics.\n",
    "\n",
    "Depending on the context of the problem several distance metrics can be used.\n",
    "\n",
    "Manhattan distance <br>\n",
    "Euclidean distance <br>\n",
    "Minkowski distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e542028-0e35-4d67-ab7f-82296a517369",
   "metadata": {},
   "source": [
    "Q4. What are some common hyperparameters in KNN classifiers and regressors, and how do they affect\n",
    "the performance of the model? How might you go about tuning these hyperparameters to improve\n",
    "model performance?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd152f7-d92b-4068-a92b-f15f28a36c28",
   "metadata": {},
   "source": [
    "The most important hyperparameter for KNN is the number of neighbors (n_neighbors).\n",
    "\n",
    "Test values between at least 1 and 21, perhaps just the odd numbers.\n",
    "\n",
    "n_neighbors in [1 to 21]\n",
    "It may also be interesting to test different distance metrics (metric) for choosing the composition of the neighborhood.\n",
    "\n",
    "metric in [‘euclidean’, ‘manhattan’, ‘minkowski’]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920fc95a-feb4-4311-8c77-b86ab3d8eccf",
   "metadata": {},
   "source": [
    "Q5. How does the size of the training set affect the performance of a KNN classifier or regressor? What\n",
    "techniques can be used to optimize the size of the training set?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb52f89-d7fe-4b5b-b3e6-3550cdeaddae",
   "metadata": {},
   "source": [
    "k-NN algorithm’s performance gets worse as the number of features increases. Hence, it’s affected by the curse of dimensionality. Because, in high-dimensional spaces, the k-NN algorithm faces two difficulties:\n",
    "\n",
    "It becomes computationally more expensive to compute distance and find the nearest neighbors in high-dimensional space\n",
    "Our assumption of similar points being situated closely breaks\n",
    "As the number of features increases, the distance between data points becomes less distinctive. Moreover, the total area we need to cover to find k neighbors increase.\n",
    "\n",
    "We call representing high-dimensional data in low-dimensional space with keeping the original properties dimension reduction.\n",
    "\n",
    "We can use a feature selection technique to eliminate more redundant features from the dataset. This method ensures that we only keep the most relevant features.\n",
    "\n",
    "Alternatively, we can use feature extraction to map the data into a low-dimensional space. The new feature set is not necessarily a subset of the original components. Typically, the new features are generated by combining the existing ones. For example, principal component analysis (PCA) applies linear mapping to maximize information over a minimal number of features.\n",
    "\n",
    "PCA and kernel methods are especially beneficial when the true dimensionality of the dataset is lower than the actual representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33861bd-ff63-489c-8c94-afec46a1d7f9",
   "metadata": {},
   "source": [
    "Q6. What are some potential drawbacks of using KNN as a classifier or regressor? How might you\n",
    "overcome these drawbacks to improve the performance of the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e15fab-93df-44ac-866e-c5bda96abede",
   "metadata": {},
   "source": [
    "Here are some of the disadvantages of using the k-nearest neighbors algorithm:\n",
    "Associated computation cost is high as it stores all the training data\n",
    "Requires high memory storage\n",
    "Need to determine the value of K\n",
    "Prediction is slow if the value of N is high\n",
    "Sensitive to irrelevant features\n",
    "\n",
    "Despite being the laziest among algorithms, KNN has built an impressive reputation and is a go-to algorithm for several classification and regression problems. Of course, due to its laziness, it might not be the best choice for cases involving large data sets. But it's one of the oldest, simplest, and most accurate algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fffa3a9-0c51-4709-9426-95ef9dc32036",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
