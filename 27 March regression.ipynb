{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adf48a8b-193c-4054-93bf-9931a99bfc6d",
   "metadata": {},
   "source": [
    "Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it\n",
    "represent?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fae644b-84aa-4ea1-adb3-2079332ab814",
   "metadata": {},
   "source": [
    "In linear regression, R-squared (R2) is a measure of how close the data points are to the fitted line. It is also known as the coefficient of determination.\n",
    "\n",
    "R2 = 1- sum squared regression (SSR) / total sum of squares (SST)\n",
    "where SSR is the sum of squares of residuals and SST is the total sum of squares\n",
    "\n",
    "Greater the value of SSR or sum of squared regression, better is the regression line. In other words, closer the value of SSR to SST (sum of squared total), better is the regression line. That would mean the value of R-squared to be closer to 1 as R-squared = SSR / SST\n",
    "\n",
    "Lesser the value of SSE of sum of squared residuals, better is the regression line. In other words, closer the value of SSE to zero (0), better is the regression line. That would mean that the value of R–squared is closer to 1 as R-squared = 1 – (SSE/SST)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e113b32e-c115-4407-b853-329c2f47e5df",
   "metadata": {},
   "source": [
    "Q2. Define adjusted R-squared and explain how it differs from the regular R-squared.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3009638-2e2c-4f52-9011-3991c273a49b",
   "metadata": {},
   "source": [
    "The adjusted R-squared is a modified version of R-squared that accounts for predictors that are not significant in a regression model. In other words, the adjusted R-squared shows whether adding additional predictors improve a regression model or not.\n",
    "\n",
    "Adjusted R-squared = 1 - [(1 - R-squared) * (n - 1) / (n - p - 1)]\n",
    "\n",
    "R-squared comes with an inherent problem – additional input variables will make the R-squared stay the same or increase (this is due to how the R-squared is calculated mathematically). Therefore, even if the additional input variables show no relationship with the output variables, the R-squared will increase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f99bd9-b0a2-45cf-932a-aaf0bd040f18",
   "metadata": {},
   "source": [
    "Q3. When is it more appropriate to use adjusted R-squared?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce966856-9f58-45d8-86de-5428c2352e0c",
   "metadata": {},
   "source": [
    "Adjusted R-squared is more appropriate to use when there are multiple independent variables in a linear regression model. Regular R-squared can be misleading in this case, as it can increase even if additional independent variables do not improve the fit of the model, leading to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20c7c74-c144-47d4-9256-1ba5f318e77b",
   "metadata": {},
   "source": [
    "Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics\n",
    "calculated, and what do they represent?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bb66d2-ce00-4039-85a4-5de208360229",
   "metadata": {},
   "source": [
    "RMSE, MSE, and MAE are common metrics used to evaluate the performance of regression models. These metrics provide a measure of how well the model is able to predict the dependent variable based on the independent variables.\n",
    "\n",
    "Root Mean Squared Error (RMSE): RMSE is a measure of the average distance between the predicted and actual values of the dependent variable. It is calculated as the square root of the mean squared error (MSE) and is expressed in the same units as the dependent variable. The formula for calculating RMSE is:\n",
    "\n",
    "RMSE = sqrt(1/n * sum((y_pred - y_actual)^2))\n",
    "\n",
    "where n is the number of observations, y_pred is the predicted value of the dependent variable, and y_actual is the actual value of the dependent variable.\n",
    "\n",
    "Mean Squared Error (MSE): MSE is the average of the squared differences between the predicted and actual values of the dependent variable. It is calculated as:\n",
    "\n",
    "MSE = 1/n * sum((y_pred - y_actual)^2)\n",
    "\n",
    "where n is the number of observations, y_pred is the predicted value of the dependent variable, and y_actual is the actual value of the dependent variable.\n",
    "\n",
    "MSE is useful in evaluating the accuracy of a regression model as it penalizes larger errors more than smaller errors.\n",
    "\n",
    "Mean Absolute Error (MAE): MAE is a measure of the average distance between the predicted and actual values of the dependent variable. It is calculated as the average of the absolute differences between the predicted and actual values of the dependent variable. The formula for calculating MAE is:\n",
    "\n",
    "MAE = 1/n * sum(abs(y_pred - y_actual))\n",
    "\n",
    "where n is the number of observations, y_pred is the predicted value of the dependent variable, and y_actual is the actual value of the dependent variable.\n",
    "\n",
    "MAE is less sensitive to outliers than MSE and is useful in situations where large errors are not significantly more important than small errors.\n",
    "\n",
    "In general, lower values of RMSE, MSE, and MAE indicate better performance of the regression model in predicting the dependent variable. However, the choice of which metric to use depends on the specific context of the analysis and the relative importance of large and small errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf965bd-6fc9-450d-a7fc-b24ef0b59a4d",
   "metadata": {},
   "source": [
    "Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in\n",
    "regression analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514c8ccb-6086-4753-9377-924eece0ef90",
   "metadata": {},
   "source": [
    "Mean Squared Error(MSE) and Root Mean Square Error penalizes the large prediction errors vi-a-vis Mean Absolute Error (MAE). However, RMSE is widely used than MSE to evaluate the performance of the regression model with other random models as it has the same units as the dependent variable (Y-axis).\n",
    "\n",
    "MSE is a differentiable function that makes it easy to perform mathematical operations in comparison to a non-differentiable function like MAE. Therefore, in many models, RMSE is used as a default metric for calculating Loss Function despite being harder to interpret than MAE.\n",
    "\n",
    "The lower value of MAE, MSE, and RMSE implies higher accuracy of a regression model. However, a higher value of R square is considered desirable.\n",
    "\n",
    "R Squared & Adjusted R Squared are used for explaining how well the independent variables in the linear regression model explains the variability in the dependent variable. R Squared value always increases with the addition of the independent variables which might lead to the addition of the redundant variables in our model. However, the adjusted R-squared solves this problem.\n",
    "\n",
    "Adjusted R squared takes into account the number of predictor variables, and it is used to determine the number of independent variables in our model. The value of Adjusted R squared decreases if the increase in the R square by the additional variable isn’t significant enough.\n",
    "For comparing the accuracy among different linear regression models, RMSE is a better choice than R Squared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56ce657-ef46-44d0-8fdf-7f656706896a",
   "metadata": {},
   "source": [
    "Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is\n",
    "it more appropriate to use?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4153b35f-9a9b-4e36-b907-7f5d41bdbd54",
   "metadata": {},
   "source": [
    "Ridge Regression:\n",
    "\n",
    "Performs L2 regularization, i.e., adds penalty equivalent to the square of the magnitude of coefficients\n",
    "Minimization objective = LS Obj + α * (sum of square of coefficients)\n",
    "\n",
    "Lasso Regression:\n",
    "\n",
    "Performs L1 regularization, i.e., adds penalty equivalent to the absolute value of the magnitude of coefficients\n",
    "Minimization objective = LS Obj + α * (sum of the absolute value of coefficients)\n",
    "Here, LS Obj refers to the ‘least squares objective,’ i.e., the linear regression objective without regularization.\n",
    "\n",
    "Lasso regularization is more appropriate to use when the dataset has many independent variables, some of which may not be relevant to the dependent variable. By eliminating these irrelevant variables from the model, Lasso regularization can improve the model's predictive accuracy and reduce overfitting. In contrast, Ridge regularization is more suitable when all the independent variables are important and have a meaningful impact on the dependent variable.\n",
    "\n",
    "In summary, Lasso regularization is a useful technique for feature selection in linear regression models with many independent variables, while Ridge regularization is more appropriate when all independent variables are important and should be included in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a87aea-382b-4fab-b0c8-5d8d1c4ee6a9",
   "metadata": {},
   "source": [
    "Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an\n",
    "example to illustrate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923e37d3-2975-4ad0-8def-f582371a5d92",
   "metadata": {},
   "source": [
    "Regularized linear models, such as Ridge and Lasso regression, help prevent overfitting in machine learning by introducing a penalty term in the objective function that penalizes large coefficients of the independent variables. The penalty term helps to reduce the complexity of the model, and by doing so, it improves the model's ability to generalize to new, unseen data.\n",
    "\n",
    "Here's an example to illustrate how regularized linear models prevent overfitting:\n",
    "\n",
    "Suppose we have a dataset with two independent variables, X1 and X2, and a dependent variable Y. We fit a linear regression model to the dataset using both X1 and X2 as predictors, and we obtain the following model:\n",
    "\n",
    "Y = 3X1 + 5X2 - 2\n",
    "\n",
    "The model has an R-squared value of 0.95, indicating a good fit to the training data. However, when we apply the model to new, unseen data, we obtain poor performance, with an R-squared value of 0.20. This is an indication of overfitting, where the model is too complex and has learned the noise in the training data.\n",
    "\n",
    "To prevent overfitting, we can use a regularized linear model, such as Ridge regression. Ridge regression introduces a penalty term in the objective function that shrinks the coefficients of the independent variables towards zero. By doing so, Ridge regression reduces the complexity of the model, which helps prevent overfitting.\n",
    "\n",
    "We fit a Ridge regression model to the same dataset and obtain the following model:\n",
    "\n",
    "Y = 2.5X1 + 3.2X2 - 0.5\n",
    "\n",
    "The model has a lower R-squared value of 0.85, indicating a slightly worse fit to the training data. However, when we apply the model to new, unseen data, we obtain much better performance, with an R-squared value of 0.80. This is an indication that the model has generalized better to the new data, and it is less prone to overfitting.\n",
    "\n",
    "In summary, regularized linear models help prevent overfitting by reducing the complexity of the model and introducing a penalty term that shrinks the coefficients of the independent variables. This improves the model's ability to generalize to new, unseen data and produces more reliable and accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047dd20f-33ce-401c-9480-7913bff3aca6",
   "metadata": {},
   "source": [
    "Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best\n",
    "choice for regression analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55604d8-97bb-4458-bf8f-5b888e2aef56",
   "metadata": {},
   "source": [
    "Here are some limitations of regularized linear models:\n",
    "\n",
    "Limited feature selection: Regularized linear models shrink the coefficients of the independent variables towards zero, which can help to eliminate irrelevant or redundant features. However, they do not perform explicit feature selection, meaning that they may not always identify the best subset of features for a given problem.\n",
    "\n",
    "Bias-variance tradeoff: Regularized linear models introduce a bias term into the model that can reduce the model's ability to fit the training data accurately. This bias term helps prevent overfitting, but it can also limit the model's flexibility and ability to capture complex patterns in the data.\n",
    "\n",
    "Non-linear relationships: Regularized linear models are limited to linear relationships between the independent variables and the dependent variable. They may not capture non-linear relationships between the variables, which can result in poor model performance.\n",
    "\n",
    "Interpretable but not always intuitive: Regularized linear models can be interpreted easily because they produce simple, linear equations. However, they may not always be intuitive, especially in cases where there are complex relationships between the variables.\n",
    "\n",
    "Sensitive to outliers: Regularized linear models are sensitive to outliers in the data. Outliers can have a large effect on the coefficients of the independent variables, which can bias the model's predictions.\n",
    "\n",
    "In summary, regularized linear models have limitations that may make them less suitable for certain regression analysis tasks. They may struggle with feature selection, non-linear relationships, and outliers, and they introduce a bias term that can limit the model's flexibility. Therefore, it is important to consider the specific characteristics of the dataset and the research question when selecting a regression analysis method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e880e6d7-52b5-459c-a05f-d6e477294a70",
   "metadata": {},
   "source": [
    "Q9. You are comparing the performance of two regression models using different evaluation metrics.\n",
    "Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better\n",
    "performer, and why? Are there any limitations to your choice of metric?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874753f0-3727-4202-85aa-85717c970e06",
   "metadata": {},
   "source": [
    "Comparing the performance of two regression models using different evaluation metrics can be challenging because different metrics emphasize different aspects of the model's performance. In this case, Model A has an RMSE of 10, while Model B has an MAE of 8.\n",
    "\n",
    "RMSE measures the *average magnitude of the residuals or prediction errors in the units of the dependent variable. It gives more weight to larger errors and is more sensitive to outliers. On the other hand, MAE measures the average absolute difference between the predicted and actual values, regardless of the direction of the errors. It is less sensitive to outliers and penalizes all errors equally*.\n",
    "\n",
    "Therefore, the choice of the better model depends on the specific characteristics of the dataset and the research question. In general, if the goal is to minimize the overall prediction error and the dataset does not have significant outliers, then Model A with an RMSE of 10 may be a better choice. On the other hand, if the goal is to minimize the average absolute error and the dataset has significant outliers, then Model B with an MAE of 8 may be more appropriate.\n",
    "\n",
    "However, it is important to note that both metrics have limitations. For example, RMSE and MAE do not provide information about the direction of the errors or the bias of the model. Additionally, both metrics assume that the errors are normally distributed and independent, which may not always be the case in real-world datasets. Therefore, it is important to use multiple evaluation metrics and assess the model's performance from different perspectives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b4bf29-a375-409a-a9f2-ea2d0722141d",
   "metadata": {},
   "source": [
    "Q10. You are comparing the performance of two regularized linear models using different types of\n",
    "regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B\n",
    "uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the\n",
    "better performer, and why? Are there any trade-offs or limitations to your choice of regularization\n",
    "method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba35f99a-e872-4a1d-aa57-bf2690aa5a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "Comparing the performance of two regularized linear models using different types of regularization can be challenging because different types of regularization emphasize different aspects of the model's performance. In this case, Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B uses Lasso regularization with a regularization parameter of 0.5.\n",
    "\n",
    "Ridge regularization shrinks the coefficients towards zero and reduces their variance, while Lasso regularization can set some of the coefficients to zero and performs feature selection. Therefore, the choice of the better model depends on the specific characteristics of the dataset and the research question.\n",
    "\n",
    "If the goal is to balance the bias-variance trade-off and minimize the overall prediction error, then Model A with Ridge regularization may be a better choice, especially if the dataset has many correlated features. Ridge regularization can effectively reduce the impact of correlated features on the model's performance and improve its generalization ability.\n",
    "\n",
    "On the other hand, if the goal is to perform feature selection and identify the most important features for the prediction task, then Model B with Lasso regularization may be more appropriate. Lasso regularization can set some of the coefficients to zero and remove irrelevant features, which can improve the model's interpretability and reduce its complexity.\n",
    "\n",
    "However, both types of regularization have limitations. Ridge regularization can only shrink the coefficients towards zero but cannot set them exactly to zero, which may not be desirable in some cases. Lasso regularization can set some coefficients to exactly zero, which can perform feature selection, but it may not be effective in the presence of highly correlated features, as it tends to select only one feature among them.\n",
    "\n",
    "In summary, the choice of the better model depends on the specific characteristics of the dataset and the research question, and both types of regularization have trade-offs and limitations that need to be considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35a0257-1500-4adf-a865-2eb8910a0c6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
