{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1aaba199-2c5e-4c9f-92a9-d232d2fa25f4",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba2a9d0-e289-405e-9534-28c19f8923c1",
   "metadata": {},
   "source": [
    " Way of data scaling, where the minimum of feature is made equal to zero and the maximum of feature equal to one. MinMax Scaler shrinks the data within the given range, usually of 0 to 1. It transforms data by scaling features to a given range. It scales the values to a specific value range without changing the shape of the original distribution.\n",
    "The MinMax scaling is done using:\n",
    "\n",
    "x_std = (x – x.min(axis=0)) / (x.max(axis=0) – x.min(axis=0))\n",
    "\n",
    "x_scaled = x_std * (max – min) + min\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29357a4f-b72b-42bb-a0ef-57d280ebe22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.        ]\n",
      " [0.27272727 0.625     ]\n",
      " [0.         1.        ]\n",
      " [1.         0.75      ]]\n"
     ]
    }
   ],
   "source": [
    "# import module\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    " \n",
    "# create data\n",
    "data = [[11, 2], [3, 7], [0, 10], [11, 8]]\n",
    " \n",
    "# scale features\n",
    "scaler = MinMaxScaler()\n",
    "model=scaler.fit(data)\n",
    "scaled_data=model.transform(data)\n",
    " \n",
    "# print scaled features\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8c3edd-8720-4322-8cf8-50d587710822",
   "metadata": {},
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f61b64-6db0-4092-a797-681b0d89db3b",
   "metadata": {},
   "source": [
    "Scaling is done considering the whole feature vector to be of unit length. This usually means dividing each component by the Euclidean length of the vector (L2 Norm). In some applications (e.g., histogram features), it can be more practical to use the L1 norm of the feature vector.\n",
    "Like Min-Max Scaling, the Unit Vector technique produces values of range [0,1]. When dealing with features with hard boundaries, this is quite useful. For example, when dealing with image data, the colors can range from only 0 to 255.\n",
    "\n",
    "The Unit Vector technique rescales the features of a dataset to have unit norm (i.e., length or magnitude of 1). This ensures that each feature contributes equally to the analysis, regardless of its scale. This technique is useful when you want to preserve the direction of the data and focus on the relative importance of each feature.\n",
    "\n",
    "On the other hand, Min-Max scaling rescales the features to a fixed range, usually between 0 and 1. To apply Min-Max scaling, you first subtract the minimum value of the feature from each value in the feature vector, then divide the result by the range (i.e., the difference between the maximum and minimum values). This technique is useful when you want to normalize the data to a specific range and ensure that all features have the same scale.\n",
    "\n",
    "Suppose we have a dataset with two features: age and income. Age ranges from 18 to 65, while income ranges from 20,000 to 200,000. We want to scale these features so that they have equal weights in our analysis.\n",
    "\n",
    "First, we need to calculate the norm for each data point. For a data point with age = 25 and income = 50,000, the norm would be:\n",
    "\n",
    "norm = sqrt(age^2 + income^2) = sqrt(25^2 + 50000^2) = 50,001\n",
    "\n",
    "Next, we need to divide each feature by the norm to get the unit vector. For this data point, the unit vector would be:\n",
    "\n",
    "age_unit = age / norm = 25 / 50,001 = 0.0005 income_unit = income / norm = 50,000 / 50,001 = 0.9999\n",
    "\n",
    "The unit vector represents the direction of the data point, while its length is always 1. By applying the Unit Vector technique, we've rescaled the features so that they have equal weights in our analysis, regardless of their original scales. We can now use these scaled features for any analysis or modeling we want to perform on the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37344241-8055-4b2b-adc0-f4ae4d898fd2",
   "metadata": {},
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a5526c-02ba-4555-9779-ba3920203e51",
   "metadata": {},
   "source": [
    "Principal component analysis, or PCA, is a statistical procedure that allows you to summarize the information content in large data tables by means of a smaller set of “summary indices” that can be more easily visualized and analyze\n",
    "\n",
    "Analysts looking for patterns and trends in stock prices have an extensive dataset containing numerous stocks along with dozens of variables, including closing price, trading volume, earnings per share, market liquidity and volatility, GDP, inflation, company earnings and revenue, dividend yield, international conditions, supply and demand factors, competition, and so on.\n",
    "\n",
    "Principal components can take the multitude of variables and reduce them to the most important indices. This method finds a smaller set of values that explain most of the variation in stock prices. Importantly, PCA ranks the components by importance, helping you know which ones to focus on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ef7d2f-4644-4fa6-92c0-435a077fdf61",
   "metadata": {},
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e51d4c-5971-4b91-aabf-09a7478a40f5",
   "metadata": {},
   "source": [
    "PCA, is a dimensionality-reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set.\n",
    "\n",
    "Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process.\n",
    "\n",
    "So, to sum up, the idea of PCA is simple — reduce the number of variables of a data set, while preserving as much information as possible.\n",
    "\n",
    "For example, after applying PCA, you may find that the first principal component is strongly correlated with the overall market trend. This means that the performance of the portfolio is heavily influenced by the general market conditions. The second principal component may be related to the performance of a specific sector, such as technology stocks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d913054-1554-4739-8c11-620aed387717",
   "metadata": {},
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec0a029-a408-428e-b7d2-ffe1f460e6f1",
   "metadata": {},
   "source": [
    "For a food delivery system, there are 3 criteria:\n",
    "price : Prices on an avg will be in range of 100 to 5000 (in INR)\n",
    "rating: It will be on a scale of 1 to 5 or 1 to 10\n",
    "delivery time: Time will be in range of 10 to 60 (mins)\n",
    "So for this dataset the prices will have huge impact on recommendation system therefore it needs to be scaled down. All 3 features can be scaled to 0 to 1 using min max scaling.\n",
    "Formula: Value - min / max - min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bce056-f164-4308-9d39-a47fee69f8fe",
   "metadata": {},
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a5abdb-e5b3-45a7-941c-38d4a1974129",
   "metadata": {},
   "source": [
    "PCA is generally applied to data points that have associated features that are independent of time. Each time series realization (or percent change of value per day) is considered to be another feature of the stock. i.e. We are treating all realizations of percent change in value of stocks to be independent of each other. This may not be a good modeling assumption since there may be temporal influence.\n",
    "\n",
    "When building a model to predict stock prices, we might have a dataset with many features, such as company financial data and market trends. However, using all of these features in a machine learning model can lead to the \"curse of dimensionality,\" where the model becomes overly complex and overfit to the training data. To address this issue, we can use Principal Component Analysis (PCA) to reduce the dimensionality of the dataset.\n",
    "\n",
    "Here's how we would use PCA to reduce the dimensionality of the dataset:\n",
    "\n",
    "Standardize the data: We first standardize the data by scaling each feature to have a mean of 0 and a standard deviation of 1. This ensures that all features are on the same scale and have equal importance in the analysis.\n",
    "\n",
    "Calculate the covariance matrix: We then calculate the covariance matrix of the standardized data, which measures the relationships between the different features.\n",
    "\n",
    "Perform eigendecomposition: We perform eigendecomposition on the covariance matrix to calculate the principal components of the data. Each principal component is a linear combination of the original features and represents a different axis in the data. The first principal component explains the most variance in the data, the second explains the second-most variance, and so on.\n",
    "\n",
    "Choose the number of principal components: We can use the scree plot or cumulative variance plot to decide how many principal components to keep. For example, we might decide to keep the first 10 principal components, which explain 80% of the variance in the data.\n",
    "\n",
    "Transform the data: Finally, we transform the original features into the new principal components, which represent the most important features in the data. These new features can then be used as input for machine learning algorithms.\n",
    "\n",
    "By using PCA to reduce the dimensionality of the dataset, we can improve the performance of machine learning algorithms by focusing on the most important features that explain the most variance in the data. This can help us build a more accurate and efficient model to predict stock prices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c80d43-4ed8-419a-9da3-e37dba8356b8",
   "metadata": {},
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14f7ca8e-0a26-4b7d-9872-0eb983ce02f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61448f8f-346b-417e-9888-678473e1f72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max=MinMaxScaler(feature_range=(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f73ef6b-515f-4e4b-9142-2c296568d024",
   "metadata": {},
   "outputs": [],
   "source": [
    "values=[1, 5, 10, 15, 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69ddb960-55cf-4b6c-a4c2-4ff45b12c1df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0\n",
       "0   1\n",
       "1   5\n",
       "2  10\n",
       "3  15\n",
       "4  20"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame(data=values)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0f9062a-1c71-4532-aa10-62429dd8a525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.578947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.052632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.473684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0\n",
       "0 -1.000000\n",
       "1 -0.578947\n",
       "2 -0.052632\n",
       "3  0.473684\n",
       "4  1.000000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scale = pd.DataFrame(min_max.fit_transform(df[[0]]))\n",
    "df_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1f603d-c494-4a48-840b-28e5dff42174",
   "metadata": {},
   "source": [
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19ab083-2633-44c5-ad01-b64771f9ddb6",
   "metadata": {},
   "source": [
    "Method 1: If your sole intention of doing PCA is for data visualization, you should select 2 or 3 principal components.\n",
    "PCA is extremely useful for data visualization. Visualization of high-dimensional data can be achieved through PCA.\n",
    "Since we are only familiar with 2D and 3D plots, we should convert high-dimensional data into 2 or 3-dimensional data to visualize them on 2D or 3D plots. This can be achieved through PCA.\n",
    "\n",
    "Method 2: If you want an exact amount of variance to be kept in data after applying PCA, specify a float between 0 and 1 to the hyperparameter n_components.\n",
    "\n",
    "Method 3: Plot the explained variance percentage of individual components and the percentage of total variance captured by all principal components.\n",
    "This is the most advanced and effective method that can be used to select the best number of principal components for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343edb85-4b89-42d4-8a23-2335805e8978",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
