{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2222427d-90ec-483f-9cbd-64cec720c64f",
   "metadata": {},
   "source": [
    "Q1. Explain the difference between linear regression and logistic regression models. Provide an example of\n",
    "a scenario where logistic regression would be more appropriate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e387a6e-b50a-45f5-9c6a-51fdc54a7393",
   "metadata": {},
   "source": [
    "Linear regression is used to predict the continuous dependent variable using a given set of independent variables.\tLogistic Regression is used to predict the categorical dependent variable using a given set of independent variables.\n",
    "\n",
    "Linear Regression is used for solving Regression problem.\tLogistic regression is used for solving Classification problems.\n",
    "\n",
    "In Linear regression, we predict the value of continuous variables.\tIn logistic Regression, we predict the values of categorical variables.\n",
    "\n",
    "In linear regression, we find the best fit line, by which we can easily predict the output.\tIn Logistic Regression, we find the S-curve by which we can classify the samples. Least square estimation method is used for estimation of accuracy. Maximum likelihood estimation method is used for estimation of accuracy.\n",
    "\n",
    "The output for Linear Regression must be a continuous value, such as price, age, etc.\tThe output of Logistic Regression must be a Categorical value such as 0 or 1, Yes or No, etc.\n",
    "\n",
    "In Linear regression, it is required that relationship between dependent variable and independent variable must be linear.\tIn Logistic regression, it is not required to have the linear relationship between the dependent and independent variable.\n",
    "\n",
    "In linear regression, there may be collinearity between the independent variables.\tIn logistic regression, there should not be collinearity between the independent variable.\n",
    "\n",
    "Given a list of grocery items, you can separate them into different categories like vegetables, fruits, dairy products, groceries, etc., using classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e64b845-f2de-4aee-b2ce-3289a4a79272",
   "metadata": {},
   "source": [
    "Q2. What is the cost function used in logistic regression, and how is it optimized?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81430e1d-22e7-4608-8ffe-e07f05881a2d",
   "metadata": {},
   "source": [
    "Logistic Regression is one of the simplest classification algorithms which we learn while exploring machine learning algorithms. But we use cross entropy instead of the mean squared error.\n",
    "\n",
    "We can also view this as a non-linear transformation of the linear regression line. By using this we get the values confined between the range 0 and 1. Also, our target class is also 0 and 1 so, the values which we get are between this range, and by applying some thresholding(if the predicted value is greater than 0.5 then predict 1 else 0) we can set the predicted values to either 0 or 1.\n",
    "\n",
    "Log loss is a classification evaluation metric that is used to compare different models which we build during the process of model development. It is considered one of the efficient metrics for evaluation purposes while dealing with the soft probabilities predicted by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131c59fc-666d-429d-afe4-9f6450ee76d2",
   "metadata": {},
   "source": [
    "Q3. Explain the concept of regularization in logistic regression and how it helps prevent overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d3e167-c4c4-4bbe-bc80-c6a429aec911",
   "metadata": {},
   "source": [
    "In most of the real world cases, the data set will have many more features and the decision boundary is more complicated. With so many features, we often overfit the data. Overfitting is a modeling error in a function that is closely fit to a data set. It captures the noise in the data set, and may not fit new incoming data.\n",
    "To overcome this issue, we mainly have two choices: 1) remove less useful features, 2) use regularization\n",
    "Regularization is very useful in overcoming overfitting. It allows us to retain even slightly useful features and automatically reduces the coefficient of those features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9169328-0df0-4f6d-b3f3-1805cbc491e3",
   "metadata": {},
   "source": [
    "Q4. What is the ROC curve, and how is it used to evaluate the performance of the logistic regression\n",
    "model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587aec39-ea15-4e7d-8478-2301c00e5863",
   "metadata": {},
   "source": [
    "ROC curves in logistic regression are used for determining the best cutoff value for predicting whether a new observation is a \"failure\" (0) or a \"success\" (1). If you're not familiar with ROC curves, they can take some effort to understand.\n",
    "\n",
    "Whatever cutoff you choose, a certain number of the rows of data will be correctly classified (you predicted the correct value for that row), and a certain number will be misclassified. Sensitivity and specificity are two metrics for evaluating the proportion of true positives and true negatives, respectively. In other words, sensitivity is the proportion of 1s that you correctly identified as 1s using that particular cutoff value, or the true positive rate. Conversely, specificity is the proportion of 0s that you correctly identified as 0s, or the true negative rate.\n",
    "\n",
    "For every point on the ROC curve (representing a different cutoff value), the location of that point is plotted as the sensitivity at that cutoff value on the Y axis, and 1 – specificity at that cutoff value on the X axis. As such, the ROC curve shows graphically the tradeoff that occurs between trying to maximize the true positive rate vs. trying to minimize the false positive rate. In an ideal situation, you would have sensitivity and specificity near 100% at all cutoffs, meaning you predict perfectly in all cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5a7909-da59-4793-aa93-b9e30fb07c0f",
   "metadata": {},
   "source": [
    "Q5. What are some common techniques for feature selection in logistic regression? How do these\n",
    "techniques help improve the model's performance?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b13319-2f92-4a52-b0b8-96ebe6afb68a",
   "metadata": {},
   "source": [
    "There are several techniques that can be used to select the best features for a logistic regression model. One common method is to use a method called “RFE”\n",
    "Recursive Feature Elimination (RFE) is a feature selection technique that involves training a model on a subset of the features, and then iteratively removing the least important features one by one until we are left with the desired number of features.\n",
    "The idea behind RFE is to determine the contribution of each feature to the model by measuring how well the model performs when that feature is removed. The features that are most important to the model will have the greatest impact on performance when they are removed.\n",
    "\n",
    "Wrapper approach to feature selection involves using a search algorithm to find the best combination of features for a predictive model. The goal is to find the combination of features that results in the best model performance, as measured by a metric such as accuracy or F1 score.\n",
    "There are several types of search algorithms that can be used for wrapper feature selection, including forward selection, backward elimination, and genetic algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70298fb3-f842-4a52-9fea-ed35a552edc8",
   "metadata": {},
   "source": [
    "Q6. How can you handle imbalanced datasets in logistic regression? What are some strategies for dealing\n",
    "with class imbalance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe5add0-9ae4-408a-945f-571097df4cc5",
   "metadata": {},
   "source": [
    "Imbalanced dataset is a type of dataset where the distribution of labels across the dataset is not balanced i.e. the distribution is biased or skewed. Group having more data points/samples is known as majority class where the group having less data points is known as minority class. Distribution in dataset can have slight imbalance or high imbalance. It is very uncommon to have perfectly equal distribution, though most of the time the distribution is slightly skewed towards one class.\n",
    "In such scenarios, distribution is highly skewed to the extent that there can be one data point of minority class for hundreds, thousands or millions of data point of majority class. Some of the practical scenarios are:\n",
    "Credit card fraud transaction (fraud transaction are very few among all financial transaction)\n",
    "Spam classification of emails (spam emails are very few compared to regular emails)\n",
    "Machine malfunction (scenarios in which machine will malfunction are very few)\n",
    "Insurance claim prediction.\n",
    "Employee attrition (usually employee attrition rate is 20% max)\n",
    "In such cases, minority class is more important than the majority class and the motive of classifier is to effectively classify the minority class from the majority class e.g. identify fraud transaction from all transactions. Such high imbalanced distribution pose a challenge for class prediction. The reason being most of the classifiers are designed or have default values assuming equal distribution of each label. Slight imbalance does not pose any challenge and can be treated like a normal classification problem.\n",
    "\n",
    "In such cases, evaluation metrics like ROC-AUC curve are a good indicator of classifier performance. It is a measure of how good model is at distinguishing between various class. Higher the ROC-AUC score, better the model is at predicting 0s as 0s and 1s as 1s. Just to remind, ROC is a probability curve and AUC represents degree or measure of separability. Apart from this metric, we will also check on recall score, false-positive (FP) and false-negative (FN) score as we build our classifier.\n",
    "\n",
    "One of the popular techniques is up-sampling (e.g. SMOTE) in which more similar data points are added to minority class to make class distribution equal. On this up-sampled modified data, any classifier can be applied.\n",
    "n case be unbalanced label distribution, the best practice for weights is to use the inverse of the label distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f27e88a-e302-4916-90b4-ec8219678054",
   "metadata": {},
   "source": [
    "Q7. Can you discuss some common issues and challenges that may arise when implementing logistic\n",
    "regression, and how they can be addressed? For example, what can be done if there is multicollinearity\n",
    "among the independent variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e6b150-1dc4-4198-8493-7995622128eb",
   "metadata": {},
   "source": [
    "If the number of observations is lesser than the number of features, Logistic Regression should not be used, otherwise, it may lead to overfitting.\n",
    "It constructs linear boundaries.\n",
    "The major limitation of Logistic Regression is the assumption of linearity between the dependent variable and the independent variables.\n",
    "It can only be used to predict discrete functions. Hence, the dependent variable of Logistic Regression is bound to the discrete number set.\n",
    "Non-linear problems can’t be solved with logistic regression because it has a linear decision surface. Linearly separable data is rarely found in real-world scenarios.\n",
    "Logistic Regression requires average or no multicollinearity between independent variables.\n",
    "It is tough to obtain complex relationships using logistic regression. More powerful and compact algorithms such as Neural Networks can easily outperform this algorithm.\n",
    "Logistic regression is less inclined to over-fitting but it can overfit in high dimensional datasets.One may consider Regularization (L1 and L2) techniques to avoid over-fittingin these scenarios.In Linear Regression independent and dependent variables are related linearly. But Logistic Regression needs that independent variables are linearly related to the log odds (log(p/(1-p)).\n",
    "\n",
    "The first simple method is to plot the correlation matrix of all the independent variables. When you are clueless about which variables to include in the model, just do a correlation matrix and select those independent variables with high correlation with dependent variable.\n",
    "The second method to check multi-collinearity is to use the Variance Inflation Factor(VIF) for each independent variable. It is a measure of multicollinearity in the set of multiple regression variables. The higher the value of VIF the higher correlation between this variable and the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0fd36e-7e8c-442c-8345-2be9c1d3c0f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
