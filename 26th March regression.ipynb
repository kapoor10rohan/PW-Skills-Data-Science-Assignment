{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f3ba267-b47d-4f3d-a20e-74f651a27813",
   "metadata": {},
   "source": [
    "Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an\n",
    "example of each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2666d603-673c-48bb-834c-8195e297b352",
   "metadata": {},
   "source": [
    "Linear Regression\n",
    "Also called simple regression, linear regression establishes the relationship between two variables. Linear regression is graphically depicted using a straight line with the slope defining how the change in one variable impacts a change in the other. The y-intercept of a linear regression relationship represents the value of one variable when the value of the other is 0.\n",
    "\n",
    "Multiple Regression\n",
    "For complex connections between data, the relationship might be explained by more than one variable. In this case, an analyst uses multiple regression which attempts to explain a dependent variable using more than one independent variable\n",
    "\n",
    "Consider an analyst who wishes to establish a relationship between the daily change in a company's stock prices and the daily change in trading volume. Using linear regression, the analyst can attempt to determine the relationship between the two variables:\n",
    "\n",
    "Daily Change in Stock Price = (Coefficient)(Daily Change in Trading Volume) + (y-intercept)\n",
    "\n",
    "If the stock price increases  0.10 before any trades occur and increases $0.01 for every share sold, the linear regression outcome is:\n",
    "\n",
    "Daily Change in Stock Price = (0.01)(Daily Change in Trading Volume) + $0.10\n",
    "\n",
    "However, the analyst realizes there are several other factors to consider including the company's P/E ratio, dividends, and prevailing inflation rate. The analyst can perform multiple regression to determine which—and how strongly—each of these variables impacts the stock price:\n",
    "\n",
    "Daily Change in Stock Price = (Coefficient)(Daily Change in Trading Volume) + (Coefficient)(Company's P/E Ratio) + (Coefficient)(Dividend) + (Coefficient)(Inflation Rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5dc2cd-9a1f-4ef0-8ce8-e4c746e51a6e",
   "metadata": {},
   "source": [
    "Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in\n",
    "a given dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4357a1f6-b15e-424c-8672-45095f725899",
   "metadata": {},
   "source": [
    "Linear regression is an analysis that assesses whether one or more predictor variables explain the dependent (criterion) variable.  The regression has five key assumptions:\n",
    "\n",
    "There is a linear relationship between the predictors (x) and the outcome (y)\n",
    "Predictors (x) are independent and observed with negligible error\n",
    "Residual Errors have a mean value of zero\n",
    "Residual Errors have constant variance\n",
    "Residual Errors are independent from each other and predictors (x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f74bd8-baf0-465b-8afb-8fce2d904d76",
   "metadata": {},
   "source": [
    "Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using\n",
    "a real-world scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157e7574-5f3c-4005-95f6-924507525ddc",
   "metadata": {},
   "source": [
    "The slope indicates the steepness of a line and the intercept indicates the location where it intersects an axis. The slope and the intercept define the linear relationship between two variables, and can be used to estimate an average rate of change. The greater the magnitude of the slope, the steeper the line and the greater the rate of change\n",
    "\n",
    "For example, a company determines that job performance for employees in a production department can be predicted using the regression model y = 130 + 4.3x, where x is the hours of in-house training they receive (from 0 to 20) and y is their score on a job skills test. The value of the y-intercept (130) indicates the average job skill score for an employee with no training. The value of the slope (4.3) indicates that for each hour of training, the job skill score increases, on average, by 4.3 points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fae8ca3-0a47-4cf3-87ba-6b53195fc5d4",
   "metadata": {},
   "source": [
    "Q4. Explain the concept of gradient descent. How is it used in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfec853-1a3a-4367-9671-8b1bac55c67f",
   "metadata": {},
   "source": [
    "Gradient descent is an optimization algorithm which is commonly-used to train machine learning models and neural networks.  Training data helps these models learn over time, and the cost function within gradient descent specifically acts as a barometer, gauging its accuracy with each iteration of parameter updates. Until the function is close to or equal to zero, the model will continue to adjust its parameters to yield the smallest possible error. Once machine learning models are optimized for accuracy, they can be powerful tools for artificial intelligence (AI) and computer science applications.\n",
    "\n",
    "Common examples of algorithms with coefficients that can be optimized using gradient descent are Linear Regression and Logistic Regression.\n",
    "\n",
    "The evaluation of how close a fit a machine learning model estimates the target function can be calculated a number of different ways, often specific to the machine learning algorithm. The cost function involves evaluating the coefficients in the machine learning model by calculating a prediction for the model for each training instance in the dataset and comparing the predictions to the actual output values and calculating a sum or average error (such as the Sum of Squared Residuals or SSR in the case of linear regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc756ed4-2651-4f41-b5ed-14ee644d420b",
   "metadata": {},
   "source": [
    "Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6ac304-77ad-4734-9047-b48c926e8029",
   "metadata": {},
   "source": [
    "Multiple linear regression (MLR), also known simply as multiple regression, is a statistical technique that uses several explanatory variables to predict the outcome of a response variable. The goal of multiple linear regression is to model the linear relationship between the explanatory (independent) variables and response (dependent) variables. In essence, multiple regression is the extension of ordinary least-squares (OLS) regression because it involves more than one explanatory variable.\n",
    "\n",
    "The Difference Between Linear and Multiple Regression\n",
    "Ordinary linear squares (OLS) regression compares the response of a dependent variable given a change in some explanatory variables. However, a dependent variable is rarely explained by only one variable. In this case, an analyst uses multiple regression, which attempts to explain a dependent variable using more than one independent variable. Multiple regressions can be linear and nonlinear.\n",
    "\n",
    "Multiple regressions are based on the assumption that there is a linear relationship between both the dependent and independent variables. It also assumes no major correlation between the independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9242341c-2218-4d83-b838-71b6c25102dd",
   "metadata": {},
   "source": [
    "Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and\n",
    "address this issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7db5e7a-1a42-422d-b49c-d2e377f7f19b",
   "metadata": {},
   "source": [
    "Multicollinearity occurs when two or more independent variables have a high correlation with one another in a regression model, which makes it difficult to determine the individual effect of each independent variable on the dependent variable.\n",
    "\n",
    "Multicollinearity can occur due to poorly designed experiments, highly observational data, creating new variables that are dependent on other variables, including identical variables in the dataset, inaccurate use of dummy variables, or insufficient data.\n",
    "\n",
    "One method to detect multicollinearity is to calculate the variance inflation factor (VIF) for each independent variable, and a VIF value greater than 1.5 indicates multicollinearity.\n",
    "\n",
    "To fix multicollinearity, one can remove one of the highly correlated variables, combine them into a single variable, or use a dimensionality reduction technique such as principal component analysis to reduce the number of variables while retaining most of the information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864474b2-9c44-414d-bf85-ddf65ac38d4a",
   "metadata": {},
   "source": [
    "Q7. Describe the polynomial regression model. How is it different from linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade925dc-03ec-4f7b-8cad-3e966758701e",
   "metadata": {},
   "source": [
    "In polynomial regression, the relationship between the independent variable x and the dependent variable y is described as an nth degree polynomial in x. Polynomial regression, abbreviated E(y |x), describes the fitting of a nonlinear relationship between the value of x and the conditional mean of y. It usually corresponded to the least-squares method. According to the Gauss Markov Theorem, the least square approach minimizes the variance of the coefficients.\n",
    "Polynomial regression can so be categorized as follows:\n",
    "\n",
    "1. Linear – if degree as 1\n",
    "\n",
    "2. Quadratic – if degree as 2\n",
    "\n",
    "3. Cubic – if degree as 3 and goes on, on the basis of degree.\n",
    "\n",
    "This is a type of Linear Regression in which the dependent and independent variables have a curvilinear relationship and the polynomial equation is fitted to the data; we’ll go over that in more detail later in the article. Machine learning is also referred to as a subset of Multiple Linear Regression. Because we convert the Multiple Linear Regression equation into a Polynomial Regression equation by including more polynomial elements.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3016c6c-6620-4d53-9633-b244eccefaad",
   "metadata": {},
   "source": [
    "Q8. What are the advantages and disadvantages of polynomial regression compared to linear\n",
    "regression? In what situations would you prefer to use polynomial regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9ff477-07c9-4444-85bc-399c242f1f8f",
   "metadata": {},
   "source": [
    "Rather than focusing on the distinctions between linear and polynomial regression, we may comprehend the importance of polynomial regression by starting with linear regression. We build our model and realize that it performs abysmally. We examine the difference between the actual value and the best fit line we predicted, and it appears that the true value has a curve on the graph, but our line is nowhere near cutting the mean of the points. This is where polynomial regression comes into play; it predicts the best-fit line that matches the pattern of the data (curve).\n",
    "\n",
    "One important distinction between Linear and Polynomial Regression is that Polynomial Regression does not require a linear relationship between the independent and dependent variables in the data set. When the Linear Regression Model fails to capture the points in the data and the Linear Regression fails to adequately represent the optimum conclusion, Polynomial Regression is used.\n",
    "\n",
    "Advantage of Polynomial Regression\n",
    "The best approximation of the connection between the dependent and independent variables is a polynomial. It can accommodate a wide range of functions. Polynomial is a type of curve that can accommodate a wide variety of curvatures.\n",
    "\n",
    "Disadvantages of Polynomial Regression\n",
    "One or two outliers in the data might have a significant impact on the nonlinear analysis’ outcomes. These are overly reliant on outliers. Furthermore, there are fewer model validation methods for detecting outliers in nonlinear regression than there are for linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4aea770-24bb-4cfe-9a70-d511d84497b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
