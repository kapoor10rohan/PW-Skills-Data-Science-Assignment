{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bb36702-a7f4-44d8-8010-7afee9ec781d",
   "metadata": {},
   "source": [
    "Q1. What is Lasso Regression, and how does it differ from other regression techniques?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce8d437-e027-4690-9c0d-e4fceb7235d7",
   "metadata": {},
   "source": [
    "Lasso regression is a regularization technique. It is used over regression methods for a more accurate prediction. This model uses shrinkage. Shrinkage is where data values are shrunk towards a central point as the mean. The lasso procedure encourages simple, sparse models (i.e. models with fewer parameters). This particular type of regression is well-suited for models showing high levels of multicollinearity or when you want to automate certain parts of model selection, like variable selection/parameter elimination.\n",
    "\n",
    "Lasso Regression is different from ridge regression as it uses absolute coefficient values for normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0bc4e8-19c4-4406-ba8e-588d82c590f1",
   "metadata": {},
   "source": [
    "Q2. What is the main advantage of using Lasso Regression in feature selection?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99185e4f-99b0-40a2-a49e-4f125d3f3777",
   "metadata": {},
   "source": [
    "Automatic features selection. The main advantage of a LASSO regression model is that it has the ability to set the coefficients for features it does not consider interesting to zero. This means that the model does some automatic feature selection to decide which features should and should not be included on its own.\n",
    "\n",
    "Reduced overfitting. Another advantage of a LASSO regression is that the L1 penalty that is added to the model helps to prevent the model from overfitting. This makes intuitive sense because when the model sets feature coefficients to zero and effectively removes features from the model, model complexity decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a298c9e-d240-4426-8d70-4dea97af64bb",
   "metadata": {},
   "source": [
    "Q3. How do you interpret the coefficients of a Lasso Regression model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbe5ac8-0468-4b17-bea7-dd3b3f31afb6",
   "metadata": {},
   "source": [
    "Lasso regression performs L1 regularization, which adds a penalty equal to the absolute value of the magnitude of coefficients. This type of regularization can result in sparse models with few coefficients; Some coefficients can become zero and eliminated from the model. Larger penalties result in coefficient values closer to zero, which is the ideal for producing simpler models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9682f1-9d9c-4ca2-bb4b-efbb0c9113c5",
   "metadata": {},
   "source": [
    "Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the\n",
    "model's performance?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544030c3-c00a-4f15-b776-8259c3d47555",
   "metadata": {},
   "source": [
    "A tuning parameter, Œª controls the strength of the L1 penalty. Œª is basically the amount of shrinkage:\n",
    "\n",
    "When Œª = 0, no parameters are eliminated. The estimate is equal to the one found with linear regression.\n",
    "As Œª increases, more and more coefficients are set to zero and eliminated (theoretically, when Œª = ‚àû, all coefficients are eliminated).\n",
    "As Œª increases, bias increases.\n",
    "As Œª decreases, variance increases.\n",
    "If an intercept is included in the model, it is usually left unchanged.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc31ad8c-b986-4e53-bc3f-b416659f9f69",
   "metadata": {},
   "source": [
    "Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f06d6d-e54b-47f5-a599-f990eddd5e29",
   "metadata": {},
   "source": [
    "The essential part of LASSO is just adding an L1 norm of the coefficients to the main term,\n",
    "\n",
    "ùëì(ùë•,ùë¶,ùõΩ)+ùúÜ‚ÄñùõΩ‚Äñ1.\n",
    "\n",
    "There's no reason ùëì has to be a linear model. It may not have an analytic solution, or be convex, but there's nothing stopping you from trying it out, and it should still induce sparsity, contingent on a large enough lambda.\n",
    "\n",
    "Though originally defined for linear regression, lasso regularization is easily extended to other statistical models including generalized linear models, generalized estimating equations, proportional hazards models, and M-estimators. Lasso's ability to perform subset selection relies on the form of the constraint and has a variety of interpretations including in terms of geometry, Bayesian statistics and convex analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0636fa-17e7-4683-8ac6-2bd07428dfe8",
   "metadata": {},
   "source": [
    "Q6. What is the difference between Ridge Regression and Lasso Regression?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0cf645-6f4e-4d59-8237-b6d681842283",
   "metadata": {},
   "source": [
    "Ridge and Lasso regression uses two different penalty functions for regularisation. Ridge regression uses L2 on the other hand lasso regression go uses L1 regularisation technique. In ridge regression, the penalty is equal to the sum of the squares of the coefficients and in the Lasso, penalty is considered to be the sum of the absolute values of the coefficients. In lasso regression, it is the shrinkage towards zero using an absolute value (L1 penalty or regularization technique) rather than a sum of squares(L2 penalty or regularization technique).\n",
    "\n",
    "Since we know that in ridge regression the coefficients can‚Äôt be zero. Here, we either consider all the coefficients or none of the coefficients, whereas Lasso regression algorithm technique, performs both parameter shrinkage and feature selection simultaneously and automatically because it nulls out the co-efficients of collinear features. This helps to select the variable(s) out of given n variables while performing lasso regression easier and more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6b3a6b-7712-4567-bd99-3999faa6b94e",
   "metadata": {},
   "source": [
    "Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f702c26-efc6-4724-a0df-32d1fac30916",
   "metadata": {},
   "source": [
    " if you have high multicollinearity in your features, then by applying Lasso Regression you can shrink the coefficients of some of the unwanted features to 0 thus eliminating multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0c1903-1b9d-4df5-971d-d8610356fc05",
   "metadata": {},
   "source": [
    "\n",
    "Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcbf93d-509c-4b4d-b64a-b2fb8c67432f",
   "metadata": {},
   "source": [
    "To choose the optimal value of the regularization parameter (lambda) in Lasso Regression, we can use a technique called cross-validation.Cross-validation is a technique that involves partitioning the data into k-folds, where k-1 folds are used for training the model and the remaining fold is used for testing. This process is repeated k times, with each fold serving as the test set once. Cross-validation involves dividing the data into training and validation sets multiple times, and evaluating the performance of the model on the validation set for different values of lambda. The lambda value that gives the best performance on the validation set is then chosen as the optimal value.\n",
    "\n",
    "Here are the general steps for choosing the optimal value of lambda in Lasso Regression:\n",
    "\n",
    "Divide the data into training and validation sets using a cross-validation technique such as k-fold cross-validation.\n",
    "Fit the Lasso Regression model on the training set for a range of lambda values.\n",
    "Evaluate the performance of the model on the validation set for each value of lambda.\n",
    "Choose the lambda value that gives the best performance on the validation set.\n",
    "Refit the Lasso Regression model on the entire training set using the chosen lambda value.\n",
    "Evaluate the final performance of the model on a separate test set.\n",
    "The choice of the range of lambda values to try can depend on the specific problem and the size of the dataset. It's common to use a logarithmic scale for lambda, such as a sequence of values between 0.01 and 100.To choose the optimal value of lambda in Lasso Regression, we can perform k-fold cross-validation for a range of values of lambda and select the lambda value that produces the best cross-validation performance. The performance metric can be the mean squared error (MSE) or the mean absolute error (MAE) between the predicted and actual values on the test set.\n",
    "\n",
    "Note that the above steps can also be automated using algorithms such as grid search or randomized search to search for the optimal value of lambda over a larger range of values.\n",
    "\n",
    "It's also important to note that the optimal value of lambda may vary depending on the size of the dataset, the number of input features, and the level of multicollinearity among the input features. Therefore, it's important to carefully tune the value of lambda for each specific problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6bff83-d590-4ebd-819f-1259b85927a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
