{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f530935c-7740-42bc-a2c9-e1922ba9609d",
   "metadata": {},
   "source": [
    "Q1. What is an ensemble technique in machine learning?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7663adda-686d-4d9f-be17-7de3007f68db",
   "metadata": {},
   "source": [
    " Ensemble methods is a machine learning technique that combines several base models in order to produce one optimal predictive model. The goal of any machine learning problem is to find a single model that will best predict our wanted outcome. Rather than making one model and hoping this model is the best/most accurate predictor we can make, ensemble methods take a myriad of models into account, and average those models to produce one final model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494d27d1-b25f-4090-9b48-dcd62f72b6b3",
   "metadata": {},
   "source": [
    "Q2. Why are ensemble techniques used in machine learning?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e48dacc-839c-4e62-9a71-5abb13a283dc",
   "metadata": {},
   "source": [
    "The goal of any machine learning problem is to find a single model that will best predict our wanted outcome. Rather than making one model and hoping this model is the best/most accurate predictor we can make, ensemble methods take a myriad of models into account, and average those models to produce one final model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6a8980-9e85-420a-84cb-1ccc85a639df",
   "metadata": {},
   "source": [
    "Q3. What is bagging?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706ad0ef-f15b-4ef9-8d99-c7be6e8eee60",
   "metadata": {},
   "source": [
    "BAGGing, or Bootstrap AGGregating. BAGGing gets its name because it combines Bootstrapping and Aggregation to form one ensemble model. Given a sample of data, multiple bootstrapped subsamples are pulled. A Decision Tree is formed on each of the bootstrapped subsamples. After each subsample Decision Tree has been formed, an algorithm is used to aggregate over the Decision Trees to form the most efficient predictor. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d05cbcf-a98a-4486-80cd-901ff78b98ba",
   "metadata": {},
   "source": [
    "Q4. What is boosting?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9556cc9-f1d5-4b5a-807d-6c6dbba9c4d2",
   "metadata": {},
   "source": [
    "Boosting is a sequential process, where each subsequent model attempts to correct the errors of the previous model. The succeeding models are dependent on the previous model. \n",
    "\n",
    "A subset is created from the original dataset.\n",
    "\n",
    "Initially, all data points are given equal weights.\n",
    "\n",
    "A base model is created on this subset.\n",
    "\n",
    "This model is used to make predictions on the whole dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a761ecf8-0fcc-45ef-9b78-ea839bfeeb76",
   "metadata": {},
   "source": [
    "Q5. What are the benefits of using ensemble techniques?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e6ac16-0528-4747-a948-802194350ca8",
   "metadata": {},
   "source": [
    "Ensemble methods offer several advantages over single models, such as improved accuracy and performance, especially for complex and noisy problems. They can also reduce the risk of overfitting and underfitting by balancing the trade-off between bias and variance, and by using different subsets and features of the data. Furthermore, ensemble methods can handle different types of data and tasks, such as classification, regression, clustering, and anomaly detection, by using different types of base models and aggregation methods. Additionally, they can provide more confidence and reliability by measuring the diversity and agreement of the base models, and by providing confidence intervals and error estimates for the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cd2971-a690-408c-8f41-76f7484d6bf1",
   "metadata": {},
   "source": [
    "Q6. Are ensemble techniques always better than individual models?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6914383b-4303-4a27-908d-acab744249cc",
   "metadata": {},
   "source": [
    "Some models naturally have a high bias or a high variance, which can be often relaxed or increased using hyperparameters that change the learning behavior of the algorithm.\n",
    "\n",
    "Ensembles provide a way to reduce the variance of the predictions; that is the amount of error in the predictions made that can be attributed to ‚Äúvariance.‚Äù\n",
    "\n",
    "This is not always the case, but when it is, this reduction in variance, in turn, leads to improved predictive performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152d93f0-2652-4eb8-8770-9eac44393b4c",
   "metadata": {},
   "source": [
    "Q7. How is the confidence interval calculated using bootstrap?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690bf26e-5d1f-4410-bddc-dcc67f507d77",
   "metadata": {},
   "source": [
    "Confidence intervals provide a range of model skills and a likelihood that the model skill will fall between the ranges when making predictions on new data. For example, a 95% likelihood of classification accuracy between 70% and 75%.\n",
    "\n",
    "A robust way to calculate confidence intervals for machine learning algorithms is to use the bootstrap. This is a general technique for estimating statistics that can be used to calculate empirical confidence intervals, regardless of the distribution of skill scores (e.g. non-Gaussian)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b6a18c-d4f4-4cf4-811c-627714812fdf",
   "metadata": {},
   "source": [
    "Q8. How does bootstrap work and What are the steps involved in bootstrap?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbd5147-ac1f-489f-8e32-5bbdcc304f6d",
   "metadata": {},
   "source": [
    "The confidence level is the overall capture rate if the method is used many times. The sample mean will vary from sample to sample, but the method estimate ¬± margin of error is used to get an interval based on each sample. C% of these intervals capture the unknown population mean ùúá. In other words, the actual mean will be located within the interval C% of the time.\n",
    "\n",
    "Confidence interval = sample mean ¬± margin of error\n",
    "\n",
    "ÔÇß The population mean for a certain variable is estimated by computing a confidence interval for that mean.\n",
    "\n",
    "ÔÇß If several random samples were collected, the mean for that variable would be slightly different from one sample to another. Therefore, when researchers estimate population means, instead of providing only one value, they specify a range of values (or an interval) within which this mean is likely to be located.\n",
    "\n",
    "ÔÇß To obtain this confidence interval, add and subtract the margin of error from the sample mean. This result is the upper limit and the lower limit of the confidence interval. The confidence interval may be wider or narrower depending on the degree of certainty, or estimation precision, that is required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1eec58-1a4b-4822-8db8-2661aee06d0a",
   "metadata": {},
   "source": [
    "Q9. A researcher wants to estimate the mean height of a population of trees. They measure the height of a\n",
    "sample of 50 trees and obtain a mean height of 15 meters and a standard deviation of 2 meters. Use\n",
    "bootstrap to estimate the 95% confidence interval for the population mean height."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "758ca57c-9210-4e5b-9e91-8872036e2563",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generating 50 samples of heights of trees with mean of 15 and std of 2 in meters\n",
    "tree_heights = np.random.normal(15,2,50)\n",
    "\n",
    "# Number of bootstrap samples\n",
    "n_samples = 50\n",
    "\n",
    "# Bootstrap resampling with replacement\n",
    "boot_means = []\n",
    "for i in range(n_samples):\n",
    "    sample = np.random.choice(tree_heights, size=50, replace=True)\n",
    "    boot_means.append(np.mean(sample))\n",
    "\n",
    "# 95% confidence interval\n",
    "lower = np.percentile(boot_means,2.5)\n",
    "upper = np.percentile(boot_means,97.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96589e43-2287-4d3e-b991-53b8df291dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The confidence interval is: 14.52 and 15.47\n"
     ]
    }
   ],
   "source": [
    "print(f'The confidence interval is: {lower:.2f} and {upper:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3897acf8-3291-4bfc-ac79-19ad9464c211",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
