{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0f90449-a12d-4f0e-9d76-24d570d2d277",
   "metadata": {},
   "source": [
    "Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some\n",
    "algorithms that are not affected by missing values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b13ace-aff6-4460-93fe-6cd6c595213d",
   "metadata": {},
   "source": [
    "Missing Data that's Missing Completely at Random (MCAR)\n",
    "These are data that are missing completely at random. That is, the missingness is independent from the data. There is no discernible pattern to this type of data missingness.\n",
    "\n",
    "This means that you cannot predict whether the value was missing due to specific circumstances or not. They are just completely missing at random.\n",
    "\n",
    "Missing Data that's Missing at Random (MAR)\n",
    "These types of data are missing at random but not completely missing. The data's missingness is determined by the data you see.\n",
    "\n",
    "Consider for instance that you built a smart watch that can track people's heart rates every hour. Then you distributed the watch to a group of individuals to wear so you can collect data for analysis.\n",
    "\n",
    "After collecting the data, you discovered that some data were missing, which was due to some people being reluctant to wear the wristwatch at night. As a result, we can conclude that the missingness was caused by the observed data.\n",
    "\n",
    "Missing Data that's Not Missing at Random (NMAR)\n",
    "These are data that are not missing at random and are also known as ignorable data. In other words, the missingness of the missing data is determined by the variable of interest.\n",
    "\n",
    "A common example is a survey in which students are asked how many cars they own. In this case, some students may purposefully fail to complete the survey, resulting in missing values.\n",
    "\n",
    "KNN and Random Forest algorithms can also support missing values. the k-NN algorithm considers the missing values by taking the majority of the K nearest values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64680294-a290-48b3-851c-cecb97ea1814",
   "metadata": {},
   "source": [
    "Q2: List down techniques used to handle missing data. Give an example of each with python code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b9c6b7-31d2-4942-9579-3d81a985b086",
   "metadata": {},
   "source": [
    "1. Delete the Data\n",
    "The easiest method is to just simply delete the whole training examples where one or several columns have null entries.\n",
    "\n",
    "data = data.dropna()\n",
    "data.isnull().sum()\n",
    "\n",
    "2. Imputing Averages\n",
    "The next method is to assign some average value (mean, median, or mode) to the null entries\n",
    "\n",
    "data.fillna(data.mean(), inplace=True)\n",
    "\n",
    "3. Assign New Category\n",
    "A better way is to assign these NaN values their own category. As we no longer have any NaN values, machine learning algorithms can now use this dataset\n",
    "\n",
    "data['Cabin'] = data['Cabin'].fillna('Unkown')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5a4530-9591-40f8-9f4b-27ffbaf8e3e4",
   "metadata": {},
   "source": [
    "Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fb65b0-5543-4c2c-96cd-eab4691a29a9",
   "metadata": {},
   "source": [
    "A classification data set with skewed class proportions is called imbalanced. Classes that make up a large proportion of the data set are called majority classes. Those that make up a smaller proportion are minority classes.\n",
    "\n",
    "With so few positives relative to negatives, the training model will spend most of its time on negative examples and not learn enough from positive ones. For example, if your batch size is 128, many batches will have no positive examples, so the gradients will be less informative.\n",
    "\n",
    "If you have an imbalanced data set, first try training on the true distribution. If the model works well and generalizes, you're done! If not, try the following downsampling and upweighting technique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04722b6-f668-4c21-bc85-7f1acd252e0e",
   "metadata": {},
   "source": [
    "Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down-\n",
    "sampling are required.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d44c1dd-7f07-4496-a196-3e3364c3fd23",
   "metadata": {},
   "source": [
    "\n",
    "Downsampling (in this context) means training on a disproportionately low subset of the majority class examples.\n",
    "Upweighting means adding an example weight to the downsampled class equal to the factor by which you downsampled\n",
    "\n",
    "Downsample the majority class. Consider our example of the fraud data set, with 1 positive to 200 negatives. Downsampling by a factor of 20 improves the balance to 1 positive to 10 negatives (10%). Although the resulting training set is still moderately imbalanced, the proportion of positives to negatives is much better than the original extremely imbalanced proportion (0.5%).\n",
    "Upweight the downsampled class: The last step is to add example weights to the downsampled class. Since we downsampled by a factor of 20, the example weight should be 20."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f94eed3-7c06-4c26-b2d6-7b8e297765d6",
   "metadata": {},
   "source": [
    "\n",
    "Q5: What is data Augmentation? Explain SMOTE.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182f987a-a795-43f2-8bed-52c2e47d6f51",
   "metadata": {},
   "source": [
    "Data augmentation is a technique of artificially increasing the training set by creating modified copies of a dataset using existing data. It includes making minor changes to the dataset or using deep learning to generate new data points.  \n",
    "The challenge of working with imbalanced datasets is that most machine learning techniques will ignore, and in turn have poor performance on, the minority class, although typically it is performance on the minority class that is most important.\n",
    "\n",
    "One approach to addressing imbalanced datasets is to oversample the minority class. The simplest approach involves duplicating examples in the minority class, although these examples donâ€™t add any new information to the model. Instead, new examples can be synthesized from the existing examples. This is a type of data augmentation for the minority class and is referred to as the Synthetic Minority Oversampling Technique, or SMOTE for short."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939bd709-e61b-4992-b60e-1d607101c8af",
   "metadata": {},
   "source": [
    "Q6: What are outliers in a dataset? Why is it essential to handle outliers?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a243da0-2bd3-4b89-b9ba-b4d6f2e89d00",
   "metadata": {},
   "source": [
    " Data records that differ dramatically from all others, they distinguish themselves in one or more characteristics. In other words, an outlier is a value that escapes normality and can (and probably will) cause anomalies in the results obtained through algorithms and analytical systems. There, they always need some degrees of attention.\n",
    "Understanding the outliers is critical in analyzing data for at least two aspects:\n",
    "The outliers may negatively bias the entire result of an analysis;\n",
    "the behavior of outliers may be precisely what is being sought."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a411a794-6bf0-49ce-a79a-2d77f521a474",
   "metadata": {},
   "source": [
    "Q7: You are working on a project that requires analyzing customer data. However, you notice that some of\n",
    "the data is missing. What are some techniques you can use to handle the missing data in your analysis?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4cfcae-783b-47cb-9852-7d96ea7c70cc",
   "metadata": {},
   "source": [
    "1. Delete the Data\n",
    "The easiest method is to just simply delete the whole training examples where one or several columns have null entries.\n",
    "\n",
    "data = data.dropna()\n",
    "data.isnull().sum()\n",
    "\n",
    "2. Imputing Averages\n",
    "The next method is to assign some average value (mean, median, or mode) to the null entries\n",
    "\n",
    "data.fillna(data.mean(), inplace=True)\n",
    "\n",
    "3. Assign New Category\n",
    "A better way is to assign these NaN values their own category. As we no longer have any NaN values, machine learning algorithms can now use this dataset\n",
    "\n",
    "data['Cabin'] = data['Cabin'].fillna('Unkown')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e85104-9a9a-493f-a80a-098d22790b3c",
   "metadata": {},
   "source": [
    "Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are\n",
    "some strategies you can use to determine if the missing data is missing at random or if there is a pattern\n",
    "to the missing data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69743ec-2a27-426e-8776-6aa9bbf4be05",
   "metadata": {},
   "source": [
    "Missing Data that's Missing Completely at Random (MCAR)\n",
    "These are data that are missing completely at random. That is, the missingness is independent from the data. There is no discernible pattern to this type of data missingness.\n",
    "\n",
    "This means that you cannot predict whether the value was missing due to specific circumstances or not. They are just completely missing at random.\n",
    "\n",
    "Missing Data that's Missing at Random (MAR)\n",
    "These types of data are missing at random but not completely missing. The data's missingness is determined by the data you see.\n",
    "\n",
    "Consider for instance that you built a smart watch that can track people's heart rates every hour. Then you distributed the watch to a group of individuals to wear so you can collect data for analysis.\n",
    "\n",
    "After collecting the data, you discovered that some data were missing, which was due to some people being reluctant to wear the wristwatch at night. As a result, we can conclude that the missingness was caused by the observed data.\n",
    "\n",
    "Missing Data that's Not Missing at Random (NMAR)\n",
    "These are data that are not missing at random and are also known as ignorable data. In other words, the missingness of the missing data is determined by the variable of interest.\n",
    "\n",
    "A common example is a survey in which students are asked how many cars they own. In this case, some students may purposefully fail to complete the survey, resulting in missing values.\n",
    "\n",
    "KNN and Random Forest algorithms can also support missing values. the k-NN algorithm considers the missing values by taking the majority of the K nearest values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0929e2ae-5925-4ac2-a6e0-792b5c5f34b3",
   "metadata": {},
   "source": [
    "Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the\n",
    "dataset do not have the condition of interest, while a small percentage do. What are some strategies you\n",
    "can use to evaluate the performance of your machine learning model on this imbalanced dataset?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe2b69d-aaa4-405f-8054-6e9645fe94dd",
   "metadata": {},
   "source": [
    " There are several strategies that can be used to evaluate the performance of a machine learning model on an imbalanced dataset:\n",
    "\n",
    "1.Confusion matrix: A confusion matrix can be used to evaluate the performance of a classification model. It shows the number of true positives, true negatives, false positives, and false negatives.\n",
    "\n",
    "2.ROC curve: A receiver operating characteristic (ROC) curve is a plot of the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. It can be used to evaluate the performance of a binary classification model.\n",
    "\n",
    "3.Precision-Recall curve: A precision-recall curve is a plot of the precision (the number of true positives divided by the number of true positives plus false positives) against the recall (the number of true positives divided by the number of true positives plus false negatives) at various threshold settings.\n",
    "\n",
    "4.F1 score: The F1 score is the harmonic mean of precision and recall. It is a measure of a model's accuracy that takes into account both precision and recall.\n",
    "\n",
    "5.Stratified sampling: When splitting the data into training and testing sets, stratified sampling can be used to ensure that the minority class is represented in both sets.\n",
    "\n",
    "6.Resampling techniques: Resampling techniques such as oversampling the minority class (e.g. using SMOTE) or undersampling the majority class can be used to balance the dataset and improve the performance of the model.\n",
    "\n",
    "7.Cost-sensitive learning: Cost-sensitive learning is a technique that assigns different misclassification costs to different classes. In an imbalanced dataset, the cost of misclassifying the minority class may be higher than the cost of misclassifying the majority class. Cost-sensitive "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e010dd-8a9d-4df2-8934-c0c0df78e79e",
   "metadata": {},
   "source": [
    "Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is\n",
    "unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to\n",
    "balance the dataset and down-sample the majority class?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39726e97-ad5b-447e-997e-470efe878cd8",
   "metadata": {},
   "source": [
    "To balance an unbalanced dataset and down-sample the majority class, we can use the following techniques:\n",
    "\n",
    "Random undersampling: Randomly removing samples from the majority class until the dataset is balanced. This technique is simple to implement but can result in the loss of important information.\n",
    "\n",
    "Cluster centroids: In this technique, the majority class is down-sampled by identifying clusters of the majority class and replacing them with their respective centroids. This method can preserve important information, but it may not be effective if there is significant overlap between the majority and minority classes.\n",
    "\n",
    "Tomek links: Tomek links are pairs of instances from different classes that are close to each other but have different labels. In this technique, the majority class is down-sampled by removing instances that form Tomek links. This method can remove noisy and borderline instances but may not be effective if there is significant overlap between the classes.\n",
    "\n",
    "Synthetic Minority Over-sampling Technique (SMOTE): SMOTE is an oversampling technique that generates synthetic data points from the minority class. The technique works by selecting a random minority class data point and creating synthetic samples along the line segments that connect its nearest neighbors. SMOTE can be used to balance the dataset by oversampling the minority class.\n",
    "\n",
    "To down-sample the majority class after balancing the dataset, we can use any of the above techniques to create a balanced dataset, followed by randomly selecting a subset of the majority class samples to down-sample the majority class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f95d01-8ea2-413d-a030-62a263b8fdfa",
   "metadata": {},
   "source": [
    "Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a\n",
    "project that requires you to estimate the occurrence of a rare event. What methods can you employ to\n",
    "balance the dataset and up-sample the minority class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d00dfea-4e02-45c2-bd4d-a2db4b83978b",
   "metadata": {},
   "source": [
    " To balance an imbalanced dataset, up-sampling can be used for the minority class. Some methods that can be employed to up-sample the minority class are:\n",
    "\n",
    "Random over-sampling: In this method, the minority class is randomly sampled with replacement to increase its size to match that of the majority class. This method can lead to overfitting if the minority class is significantly smaller than the majority class.\n",
    "\n",
    "Synthetic Minority Over-sampling Technique (SMOTE): SMOTE creates synthetic samples by interpolating between the samples of the minority class. It randomly selects a minority class instance and computes the k nearest neighbors. Then, it creates new instances by interpolating between the chosen instance and its k nearest neighbors. SMOTE reduces the risk of overfitting and improves the generalization ability of the model.\n",
    "\n",
    "Adaptive Synthetic Sampling (ADASYN): ADASYN is an extension of SMOTE. It generates synthetic samples for the minority class by computing the density distribution of the minority class and focusing on generating samples in areas that are difficult to learn. This method can improve the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5f7b7d-d7c0-4b95-b1a2-1165fe09c2ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
